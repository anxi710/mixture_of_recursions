{
    "results": {
        "arc_challenge": {
            "acc,none": 0.19795221843003413,
            "acc_stderr,none": 0.011643990971573369,
            "acc_norm,none": 0.23208191126279865,
            "acc_norm_stderr,none": 0.012336718284948816
        },
        "arc_easy": {
            "acc,none": 0.4734848484848485,
            "acc_stderr,none": 0.010245347015573841,
            "acc_norm,none": 0.43308080808080807,
            "acc_norm_stderr,none": 0.010167478013701743
        },
        "hellaswag": {
            "acc,none": 0.28201553475403307,
            "acc_stderr,none": 0.004490612245335525,
            "acc_norm,none": 0.29635530770762797,
            "acc_norm_stderr,none": 0.004557163175885494
        },
        "lambada_openai": {
            "perplexity,none": 235.65791917036032,
            "perplexity_stderr,none": 11.727906136603984,
            "acc,none": 0.20298855035901417,
            "acc_stderr,none": 0.005603767705440447
        },
        "mmlu": {
            "acc,none": 0.2313060817547358,
            "acc_stderr,none": 0.0035534144482831163
        },
        "mmlu_humanities": {
            "acc,none": 0.24335812964930925,
            "acc_stderr,none": 0.006253968700387386
        },
        "mmlu_formal_logic": {
            "acc,none": 0.2857142857142857,
            "acc_stderr,none": 0.04040610178208843
        },
        "mmlu_high_school_european_history": {
            "acc,none": 0.22424242424242424,
            "acc_stderr,none": 0.03256866661681102
        },
        "mmlu_high_school_us_history": {
            "acc,none": 0.25980392156862747,
            "acc_stderr,none": 0.030778554678693247
        },
        "mmlu_high_school_world_history": {
            "acc,none": 0.2742616033755274,
            "acc_stderr,none": 0.029041333510597976
        },
        "mmlu_international_law": {
            "acc,none": 0.2396694214876033,
            "acc_stderr,none": 0.03896878985070412
        },
        "mmlu_jurisprudence": {
            "acc,none": 0.25925925925925924,
            "acc_stderr,none": 0.042365112580946315
        },
        "mmlu_logical_fallacies": {
            "acc,none": 0.22085889570552147,
            "acc_stderr,none": 0.032591773927421734
        },
        "mmlu_moral_disputes": {
            "acc,none": 0.2514450867052023,
            "acc_stderr,none": 0.023357365785874006
        },
        "mmlu_moral_scenarios": {
            "acc,none": 0.23798882681564246,
            "acc_stderr,none": 0.014242630070574904
        },
        "mmlu_philosophy": {
            "acc,none": 0.18971061093247588,
            "acc_stderr,none": 0.0222681962587832
        },
        "mmlu_prehistory": {
            "acc,none": 0.21604938271604937,
            "acc_stderr,none": 0.02289916291844576
        },
        "mmlu_professional_law": {
            "acc,none": 0.2457627118644068,
            "acc_stderr,none": 0.010996156635142657
        },
        "mmlu_world_religions": {
            "acc,none": 0.3216374269005848,
            "acc_stderr,none": 0.03582529442573121
        },
        "mmlu_other": {
            "acc,none": 0.24074670099774703,
            "acc_stderr,none": 0.007655291859261943
        },
        "mmlu_business_ethics": {
            "acc,none": 0.3,
            "acc_stderr,none": 0.04605661864718382
        },
        "mmlu_clinical_knowledge": {
            "acc,none": 0.2188679245283019,
            "acc_stderr,none": 0.025447863825108594
        },
        "mmlu_college_medicine": {
            "acc,none": 0.2138728323699422,
            "acc_stderr,none": 0.03126511206173045
        },
        "mmlu_global_facts": {
            "acc,none": 0.18,
            "acc_stderr,none": 0.03861229196653691
        },
        "mmlu_human_aging": {
            "acc,none": 0.3183856502242152,
            "acc_stderr,none": 0.03126580522513711
        },
        "mmlu_management": {
            "acc,none": 0.18446601941747573,
            "acc_stderr,none": 0.03840423627288276
        },
        "mmlu_marketing": {
            "acc,none": 0.2863247863247863,
            "acc_stderr,none": 0.029614323690456627
        },
        "mmlu_medical_genetics": {
            "acc,none": 0.3,
            "acc_stderr,none": 0.04605661864718382
        },
        "mmlu_miscellaneous": {
            "acc,none": 0.23627075351213284,
            "acc_stderr,none": 0.01519047371703751
        },
        "mmlu_nutrition": {
            "acc,none": 0.22549019607843138,
            "acc_stderr,none": 0.023929155517351218
        },
        "mmlu_professional_accounting": {
            "acc,none": 0.23049645390070922,
            "acc_stderr,none": 0.025123739226872346
        },
        "mmlu_professional_medicine": {
            "acc,none": 0.19117647058823528,
            "acc_stderr,none": 0.023886881922440366
        },
        "mmlu_virology": {
            "acc,none": 0.28313253012048195,
            "acc_stderr,none": 0.035072954313705176
        },
        "mmlu_social_sciences": {
            "acc,none": 0.2203444913877153,
            "acc_stderr,none": 0.007468966322753829
        },
        "mmlu_econometrics": {
            "acc,none": 0.23684210526315788,
            "acc_stderr,none": 0.03999423879281335
        },
        "mmlu_high_school_geography": {
            "acc,none": 0.17676767676767677,
            "acc_stderr,none": 0.027178752639044908
        },
        "mmlu_high_school_government_and_politics": {
            "acc,none": 0.20207253886010362,
            "acc_stderr,none": 0.028979089794296756
        },
        "mmlu_high_school_macroeconomics": {
            "acc,none": 0.2205128205128205,
            "acc_stderr,none": 0.021020672680827836
        },
        "mmlu_high_school_microeconomics": {
            "acc,none": 0.21008403361344538,
            "acc_stderr,none": 0.026461398717471864
        },
        "mmlu_high_school_psychology": {
            "acc,none": 0.1926605504587156,
            "acc_stderr,none": 0.016909276884936104
        },
        "mmlu_human_sexuality": {
            "acc,none": 0.2595419847328244,
            "acc_stderr,none": 0.03844876139785267
        },
        "mmlu_professional_psychology": {
            "acc,none": 0.25163398692810457,
            "acc_stderr,none": 0.017555818091322294
        },
        "mmlu_public_relations": {
            "acc,none": 0.21818181818181817,
            "acc_stderr,none": 0.03955932861795833
        },
        "mmlu_security_studies": {
            "acc,none": 0.18775510204081633,
            "acc_stderr,none": 0.025000256039546167
        },
        "mmlu_sociology": {
            "acc,none": 0.24875621890547264,
            "acc_stderr,none": 0.030567675938916686
        },
        "mmlu_us_foreign_policy": {
            "acc,none": 0.28,
            "acc_stderr,none": 0.045126085985421296
        },
        "mmlu_stem": {
            "acc,none": 0.2147161433555344,
            "acc_stderr,none": 0.0073008182040857844
        },
        "mmlu_abstract_algebra": {
            "acc,none": 0.22,
            "acc_stderr,none": 0.041633319989322654
        },
        "mmlu_anatomy": {
            "acc,none": 0.18518518518518517,
            "acc_stderr,none": 0.03355677216313144
        },
        "mmlu_astronomy": {
            "acc,none": 0.17763157894736842,
            "acc_stderr,none": 0.031103182383123377
        },
        "mmlu_college_biology": {
            "acc,none": 0.2638888888888889,
            "acc_stderr,none": 0.03685651095897531
        },
        "mmlu_college_chemistry": {
            "acc,none": 0.21,
            "acc_stderr,none": 0.040936018074033236
        },
        "mmlu_college_computer_science": {
            "acc,none": 0.25,
            "acc_stderr,none": 0.04351941398892446
        },
        "mmlu_college_mathematics": {
            "acc,none": 0.21,
            "acc_stderr,none": 0.040936018074033236
        },
        "mmlu_college_physics": {
            "acc,none": 0.21568627450980393,
            "acc_stderr,none": 0.04092563958237658
        },
        "mmlu_computer_security": {
            "acc,none": 0.29,
            "acc_stderr,none": 0.045604802157206865
        },
        "mmlu_conceptual_physics": {
            "acc,none": 0.25957446808510637,
            "acc_stderr,none": 0.02865917937429237
        },
        "mmlu_electrical_engineering": {
            "acc,none": 0.2482758620689655,
            "acc_stderr,none": 0.03600105692727776
        },
        "mmlu_elementary_mathematics": {
            "acc,none": 0.20899470899470898,
            "acc_stderr,none": 0.020940481565334935
        },
        "mmlu_high_school_biology": {
            "acc,none": 0.1774193548387097,
            "acc_stderr,none": 0.021732540689329255
        },
        "mmlu_high_school_chemistry": {
            "acc,none": 0.16748768472906403,
            "acc_stderr,none": 0.026273086047535348
        },
        "mmlu_high_school_computer_science": {
            "acc,none": 0.26,
            "acc_stderr,none": 0.0440844002276808
        },
        "mmlu_high_school_mathematics": {
            "acc,none": 0.2111111111111111,
            "acc_stderr,none": 0.02488211685765511
        },
        "mmlu_high_school_physics": {
            "acc,none": 0.1986754966887417,
            "acc_stderr,none": 0.032578473844367795
        },
        "mmlu_high_school_statistics": {
            "acc,none": 0.1574074074074074,
            "acc_stderr,none": 0.024837173518242432
        },
        "mmlu_machine_learning": {
            "acc,none": 0.3125,
            "acc_stderr,none": 0.043994650575715215
        },
        "mmlu_continuation": {
            "acc,none": 0.25103261643640506,
            "acc_stderr,none": 0.0036459275318879744
        },
        "humanities": {
            "acc,none": 0.23464399574920297,
            "acc_stderr,none": 0.006173318342790705
        },
        "mmlu_formal_logic_continuation": {
            "acc,none": 0.2777777777777778,
            "acc_stderr,none": 0.04006168083848877,
            "acc_norm,none": 0.23809523809523808,
            "acc_norm_stderr,none": 0.03809523809523809
        },
        "mmlu_high_school_european_history_continuation": {
            "acc,none": 0.24242424242424243,
            "acc_stderr,none": 0.03346409881055956,
            "acc_norm,none": 0.32727272727272727,
            "acc_norm_stderr,none": 0.036639749943912406
        },
        "mmlu_high_school_us_history_continuation": {
            "acc,none": 0.23529411764705882,
            "acc_stderr,none": 0.029771775228145628,
            "acc_norm,none": 0.29901960784313725,
            "acc_norm_stderr,none": 0.032133257173736156
        },
        "mmlu_high_school_world_history_continuation": {
            "acc,none": 0.24050632911392406,
            "acc_stderr,none": 0.02782078198114972,
            "acc_norm,none": 0.25738396624472576,
            "acc_norm_stderr,none": 0.028458820991460302
        },
        "mmlu_international_law_continuation": {
            "acc,none": 0.14049586776859505,
            "acc_stderr,none": 0.03172233426002156,
            "acc_norm,none": 0.23140495867768596,
            "acc_norm_stderr,none": 0.03849856098794088
        },
        "mmlu_jurisprudence_continuation": {
            "acc,none": 0.16666666666666666,
            "acc_stderr,none": 0.03602814176392641,
            "acc_norm,none": 0.2037037037037037,
            "acc_norm_stderr,none": 0.038935425188248496
        },
        "mmlu_logical_fallacies_continuation": {
            "acc,none": 0.26380368098159507,
            "acc_stderr,none": 0.03462419931615622,
            "acc_norm,none": 0.31901840490797545,
            "acc_norm_stderr,none": 0.03661997551073836
        },
        "mmlu_moral_disputes_continuation": {
            "acc,none": 0.20809248554913296,
            "acc_stderr,none": 0.02185525526342175,
            "acc_norm,none": 0.17630057803468208,
            "acc_norm_stderr,none": 0.020516425672490755
        },
        "mmlu_moral_scenarios_continuation": {
            "acc,none": 0.23798882681564246,
            "acc_stderr,none": 0.014242630070574904,
            "acc_norm,none": 0.27262569832402234,
            "acc_norm_stderr,none": 0.014893391735249532
        },
        "mmlu_philosophy_continuation": {
            "acc,none": 0.24758842443729903,
            "acc_stderr,none": 0.024513879973621984,
            "acc_norm,none": 0.26366559485530544,
            "acc_norm_stderr,none": 0.025025538500532286
        },
        "mmlu_prehistory_continuation": {
            "acc,none": 0.28703703703703703,
            "acc_stderr,none": 0.025171041915309698,
            "acc_norm,none": 0.19753086419753085,
            "acc_norm_stderr,none": 0.022152889927898985
        },
        "mmlu_professional_law_continuation": {
            "acc,none": 0.23468057366362452,
            "acc_stderr,none": 0.010824026872449544,
            "acc_norm,none": 0.25358539765319427,
            "acc_norm_stderr,none": 0.011111715336101129
        },
        "mmlu_world_religions_continuation": {
            "acc,none": 0.18128654970760233,
            "acc_stderr,none": 0.029547741687640055,
            "acc_norm,none": 0.23391812865497075,
            "acc_norm_stderr,none": 0.032467217651178305
        },
        "other": {
            "acc,none": 0.27421950434502734,
            "acc_stderr,none": 0.007981254323525604
        },
        "mmlu_business_ethics_continuation": {
            "acc,none": 0.37,
            "acc_stderr,none": 0.048523658709390974,
            "acc_norm,none": 0.27,
            "acc_norm_stderr,none": 0.04461960433384737
        },
        "mmlu_clinical_knowledge_continuation": {
            "acc,none": 0.2037735849056604,
            "acc_stderr,none": 0.024790784501775447,
            "acc_norm,none": 0.30566037735849055,
            "acc_norm_stderr,none": 0.02835329807332263
        },
        "mmlu_college_medicine_continuation": {
            "acc,none": 0.2543352601156069,
            "acc_stderr,none": 0.03320556443085566,
            "acc_norm,none": 0.21965317919075145,
            "acc_norm_stderr,none": 0.03156809362703173
        },
        "mmlu_global_facts_continuation": {
            "acc,none": 0.28,
            "acc_stderr,none": 0.045126085985421296,
            "acc_norm,none": 0.26,
            "acc_norm_stderr,none": 0.0440844002276808
        },
        "mmlu_human_aging_continuation": {
            "acc,none": 0.3183856502242152,
            "acc_stderr,none": 0.03126580522513711,
            "acc_norm,none": 0.273542600896861,
            "acc_norm_stderr,none": 0.029918586707798872
        },
        "mmlu_management_continuation": {
            "acc,none": 0.23300970873786409,
            "acc_stderr,none": 0.041858325989283136,
            "acc_norm,none": 0.3300970873786408,
            "acc_norm_stderr,none": 0.046561471100123486
        },
        "mmlu_marketing_continuation": {
            "acc,none": 0.31196581196581197,
            "acc_stderr,none": 0.030351527323344996,
            "acc_norm,none": 0.3162393162393162,
            "acc_norm_stderr,none": 0.03046365674734023
        },
        "mmlu_medical_genetics_continuation": {
            "acc,none": 0.26,
            "acc_stderr,none": 0.0440844002276808,
            "acc_norm,none": 0.3,
            "acc_norm_stderr,none": 0.04605661864718382
        },
        "mmlu_miscellaneous_continuation": {
            "acc,none": 0.31545338441890164,
            "acc_stderr,none": 0.016617501738763366,
            "acc_norm,none": 0.28735632183908044,
            "acc_norm_stderr,none": 0.016182410730682734
        },
        "mmlu_nutrition_continuation": {
            "acc,none": 0.21568627450980393,
            "acc_stderr,none": 0.02355083135199512,
            "acc_norm,none": 0.26143790849673204,
            "acc_norm_stderr,none": 0.025160998214292445
        },
        "mmlu_professional_accounting_continuation": {
            "acc,none": 0.2553191489361702,
            "acc_stderr,none": 0.026011992930901992,
            "acc_norm,none": 0.2695035460992908,
            "acc_norm_stderr,none": 0.02646903681859067
        },
        "mmlu_professional_medicine_continuation": {
            "acc,none": 0.25735294117647056,
            "acc_stderr,none": 0.02655651947004155,
            "acc_norm,none": 0.28308823529411764,
            "acc_norm_stderr,none": 0.027365861131513815
        },
        "mmlu_virology_continuation": {
            "acc,none": 0.24096385542168675,
            "acc_stderr,none": 0.033293941190735296,
            "acc_norm,none": 0.3072289156626506,
            "acc_norm_stderr,none": 0.03591566797824665
        },
        "social sciences": {
            "acc,none": 0.27819304517387067,
            "acc_stderr,none": 0.008055218397099483
        },
        "mmlu_econometrics_continuation": {
            "acc,none": 0.21929824561403508,
            "acc_stderr,none": 0.038924311065187546,
            "acc_norm,none": 0.20175438596491227,
            "acc_norm_stderr,none": 0.03775205013583637
        },
        "mmlu_high_school_geography_continuation": {
            "acc,none": 0.2878787878787879,
            "acc_stderr,none": 0.03225883512300998,
            "acc_norm,none": 0.31313131313131315,
            "acc_norm_stderr,none": 0.033042050878136546
        },
        "mmlu_high_school_government_and_politics_continuation": {
            "acc,none": 0.2694300518134715,
            "acc_stderr,none": 0.03201867122877791,
            "acc_norm,none": 0.30569948186528495,
            "acc_norm_stderr,none": 0.03324837939758158
        },
        "mmlu_high_school_macroeconomics_continuation": {
            "acc,none": 0.24871794871794872,
            "acc_stderr,none": 0.021916957709213813,
            "acc_norm,none": 0.29743589743589743,
            "acc_norm_stderr,none": 0.02317740813146596
        },
        "mmlu_high_school_microeconomics_continuation": {
            "acc,none": 0.20588235294117646,
            "acc_stderr,none": 0.026265024608275907,
            "acc_norm,none": 0.3403361344537815,
            "acc_norm_stderr,none": 0.030778057422931663
        },
        "mmlu_high_school_psychology_continuation": {
            "acc,none": 0.326605504587156,
            "acc_stderr,none": 0.02010699088993737,
            "acc_norm,none": 0.3192660550458716,
            "acc_norm_stderr,none": 0.019987829069749934
        },
        "mmlu_human_sexuality_continuation": {
            "acc,none": 0.3511450381679389,
            "acc_stderr,none": 0.041864451630137474,
            "acc_norm,none": 0.2900763358778626,
            "acc_norm_stderr,none": 0.03980066246467765
        },
        "mmlu_professional_psychology_continuation": {
            "acc,none": 0.2679738562091503,
            "acc_stderr,none": 0.01791797406959478,
            "acc_norm,none": 0.272875816993464,
            "acc_norm_stderr,none": 0.01802047414839352
        },
        "mmlu_public_relations_continuation": {
            "acc,none": 0.36363636363636365,
            "acc_stderr,none": 0.04607582090719978,
            "acc_norm,none": 0.2545454545454545,
            "acc_norm_stderr,none": 0.04172343038705381
        },
        "mmlu_security_studies_continuation": {
            "acc,none": 0.3183673469387755,
            "acc_stderr,none": 0.029822533793982028,
            "acc_norm,none": 0.22040816326530613,
            "acc_norm_stderr,none": 0.026537045312145312
        },
        "mmlu_sociology_continuation": {
            "acc,none": 0.22388059701492538,
            "acc_stderr,none": 0.029475250236017162,
            "acc_norm,none": 0.23880597014925373,
            "acc_norm_stderr,none": 0.030147775935409186
        },
        "mmlu_us_foreign_policy_continuation": {
            "acc,none": 0.25,
            "acc_stderr,none": 0.04351941398892446,
            "acc_norm,none": 0.27,
            "acc_norm_stderr,none": 0.04461960433384737
        },
        "stem": {
            "acc,none": 0.2261338407865525,
            "acc_stderr,none": 0.00742513771374862
        },
        "mmlu_abstract_algebra_continuation": {
            "acc,none": 0.2,
            "acc_stderr,none": 0.04020151261036849,
            "acc_norm,none": 0.28,
            "acc_norm_stderr,none": 0.045126085985421296
        },
        "mmlu_anatomy_continuation": {
            "acc,none": 0.28888888888888886,
            "acc_stderr,none": 0.0391545063041425,
            "acc_norm,none": 0.26666666666666666,
            "acc_norm_stderr,none": 0.03820169914517904
        },
        "mmlu_astronomy_continuation": {
            "acc,none": 0.23684210526315788,
            "acc_stderr,none": 0.03459777606810534,
            "acc_norm,none": 0.3026315789473684,
            "acc_norm_stderr,none": 0.03738520676119667
        },
        "mmlu_college_biology_continuation": {
            "acc,none": 0.2916666666666667,
            "acc_stderr,none": 0.03800968060554863,
            "acc_norm,none": 0.2708333333333333,
            "acc_norm_stderr,none": 0.0371617743756602
        },
        "mmlu_college_chemistry_continuation": {
            "acc,none": 0.24,
            "acc_stderr,none": 0.04292346959909278,
            "acc_norm,none": 0.21,
            "acc_norm_stderr,none": 0.040936018074033236
        },
        "mmlu_college_computer_science_continuation": {
            "acc,none": 0.2,
            "acc_stderr,none": 0.04020151261036849,
            "acc_norm,none": 0.21,
            "acc_norm_stderr,none": 0.040936018074033236
        },
        "mmlu_college_mathematics_continuation": {
            "acc,none": 0.16,
            "acc_stderr,none": 0.03684529491774706,
            "acc_norm,none": 0.2,
            "acc_norm_stderr,none": 0.04020151261036849
        },
        "mmlu_college_physics_continuation": {
            "acc,none": 0.17647058823529413,
            "acc_stderr,none": 0.03793281185307812,
            "acc_norm,none": 0.19607843137254902,
            "acc_norm_stderr,none": 0.03950581861179961
        },
        "mmlu_computer_security_continuation": {
            "acc,none": 0.22,
            "acc_stderr,none": 0.041633319989322654,
            "acc_norm,none": 0.27,
            "acc_norm_stderr,none": 0.04461960433384737
        },
        "mmlu_conceptual_physics_continuation": {
            "acc,none": 0.2978723404255319,
            "acc_stderr,none": 0.029896145682095487,
            "acc_norm,none": 0.23404255319148937,
            "acc_norm_stderr,none": 0.02767845257821238
        },
        "mmlu_electrical_engineering_continuation": {
            "acc,none": 0.25517241379310346,
            "acc_stderr,none": 0.036329840527078404,
            "acc_norm,none": 0.22758620689655173,
            "acc_norm_stderr,none": 0.03493950380131187
        },
        "mmlu_elementary_mathematics_continuation": {
            "acc,none": 0.2222222222222222,
            "acc_stderr,none": 0.02141168439369428,
            "acc_norm,none": 0.21428571428571427,
            "acc_norm_stderr,none": 0.021132859182754503
        },
        "mmlu_high_school_biology_continuation": {
            "acc,none": 0.25806451612903225,
            "acc_stderr,none": 0.02489246917246276,
            "acc_norm,none": 0.2870967741935484,
            "acc_norm_stderr,none": 0.02573654274559456
        },
        "mmlu_high_school_chemistry_continuation": {
            "acc,none": 0.18226600985221675,
            "acc_stderr,none": 0.02716334085964518,
            "acc_norm,none": 0.23645320197044334,
            "acc_norm_stderr,none": 0.029896114291733576
        },
        "mmlu_high_school_computer_science_continuation": {
            "acc,none": 0.19,
            "acc_stderr,none": 0.039427724440366255,
            "acc_norm,none": 0.29,
            "acc_norm_stderr,none": 0.045604802157206865
        },
        "mmlu_high_school_mathematics_continuation": {
            "acc,none": 0.12962962962962962,
            "acc_stderr,none": 0.020479910253320677,
            "acc_norm,none": 0.1814814814814815,
            "acc_norm_stderr,none": 0.023499264669407334
        },
        "mmlu_high_school_physics_continuation": {
            "acc,none": 0.26490066225165565,
            "acc_stderr,none": 0.036030385453603826,
            "acc_norm,none": 0.2781456953642384,
            "acc_norm_stderr,none": 0.03658603262763744
        },
        "mmlu_high_school_statistics_continuation": {
            "acc,none": 0.2175925925925926,
            "acc_stderr,none": 0.028139689444859672,
            "acc_norm,none": 0.2962962962962963,
            "acc_norm_stderr,none": 0.03114144782353603
        },
        "mmlu_machine_learning_continuation": {
            "acc,none": 0.24107142857142858,
            "acc_stderr,none": 0.040598672469526885,
            "acc_norm,none": 0.24107142857142858,
            "acc_norm_stderr,none": 0.040598672469526885
        },
        "openbookqa": {
            "acc,none": 0.184,
            "acc_stderr,none": 0.017346174781752842,
            "acc_norm,none": 0.292,
            "acc_norm_stderr,none": 0.020354375480530096
        },
        "piqa": {
            "acc,none": 0.6050054406964092,
            "acc_stderr,none": 0.011405665187969115,
            "acc_norm,none": 0.6055495103373232,
            "acc_norm_stderr,none": 0.011402931101558359
        },
        "winogrande": {
            "acc,none": 0.4964483030781373,
            "acc_stderr,none": 0.014052131146915853
        }
    },
    "groups": {
        "mmlu": {
            "acc,none": 0.2313060817547358,
            "acc_stderr,none": 0.0035534144482831163
        },
        "mmlu_humanities": {
            "acc,none": 0.24335812964930925,
            "acc_stderr,none": 0.006253968700387386
        },
        "mmlu_other": {
            "acc,none": 0.24074670099774703,
            "acc_stderr,none": 0.007655291859261943
        },
        "mmlu_social_sciences": {
            "acc,none": 0.2203444913877153,
            "acc_stderr,none": 0.007468966322753829
        },
        "mmlu_stem": {
            "acc,none": 0.2147161433555344,
            "acc_stderr,none": 0.0073008182040857844
        },
        "mmlu_continuation": {
            "acc,none": 0.25103261643640506,
            "acc_stderr,none": 0.0036459275318879744
        },
        "humanities": {
            "acc,none": 0.23464399574920297,
            "acc_stderr,none": 0.006173318342790705
        },
        "other": {
            "acc,none": 0.27421950434502734,
            "acc_stderr,none": 0.007981254323525604
        },
        "social sciences": {
            "acc,none": 0.27819304517387067,
            "acc_stderr,none": 0.008055218397099483
        },
        "stem": {
            "acc,none": 0.2261338407865525,
            "acc_stderr,none": 0.00742513771374862
        }
    },
    "group_subtasks": {
        "arc_challenge": [],
        "arc_easy": [],
        "hellaswag": [],
        "lambada_openai": [],
        "mmlu_humanities": [
            "mmlu_world_religions",
            "mmlu_international_law",
            "mmlu_jurisprudence",
            "mmlu_logical_fallacies",
            "mmlu_formal_logic",
            "mmlu_high_school_european_history",
            "mmlu_philosophy",
            "mmlu_prehistory",
            "mmlu_professional_law",
            "mmlu_high_school_us_history",
            "mmlu_high_school_world_history",
            "mmlu_moral_disputes",
            "mmlu_moral_scenarios"
        ],
        "mmlu_social_sciences": [
            "mmlu_us_foreign_policy",
            "mmlu_econometrics",
            "mmlu_human_sexuality",
            "mmlu_high_school_geography",
            "mmlu_high_school_government_and_politics",
            "mmlu_high_school_macroeconomics",
            "mmlu_high_school_microeconomics",
            "mmlu_professional_psychology",
            "mmlu_high_school_psychology",
            "mmlu_public_relations",
            "mmlu_security_studies",
            "mmlu_sociology"
        ],
        "mmlu_other": [
            "mmlu_business_ethics",
            "mmlu_clinical_knowledge",
            "mmlu_college_medicine",
            "mmlu_global_facts",
            "mmlu_management",
            "mmlu_marketing",
            "mmlu_human_aging",
            "mmlu_nutrition",
            "mmlu_professional_accounting",
            "mmlu_professional_medicine",
            "mmlu_medical_genetics",
            "mmlu_miscellaneous",
            "mmlu_virology"
        ],
        "mmlu_stem": [
            "mmlu_abstract_algebra",
            "mmlu_anatomy",
            "mmlu_astronomy",
            "mmlu_college_biology",
            "mmlu_college_chemistry",
            "mmlu_college_computer_science",
            "mmlu_college_mathematics",
            "mmlu_college_physics",
            "mmlu_computer_security",
            "mmlu_conceptual_physics",
            "mmlu_electrical_engineering",
            "mmlu_machine_learning",
            "mmlu_elementary_mathematics",
            "mmlu_high_school_biology",
            "mmlu_high_school_chemistry",
            "mmlu_high_school_computer_science",
            "mmlu_high_school_mathematics",
            "mmlu_high_school_physics",
            "mmlu_high_school_statistics"
        ],
        "mmlu": [
            "mmlu_stem",
            "mmlu_other",
            "mmlu_social_sciences",
            "mmlu_humanities"
        ],
        "humanities": [
            "mmlu_philosophy_continuation",
            "mmlu_prehistory_continuation",
            "mmlu_formal_logic_continuation",
            "mmlu_professional_law_continuation",
            "mmlu_logical_fallacies_continuation",
            "mmlu_high_school_european_history_continuation",
            "mmlu_world_religions_continuation",
            "mmlu_high_school_us_history_continuation",
            "mmlu_high_school_world_history_continuation",
            "mmlu_international_law_continuation",
            "mmlu_jurisprudence_continuation",
            "mmlu_moral_disputes_continuation",
            "mmlu_moral_scenarios_continuation"
        ],
        "social sciences": [
            "mmlu_econometrics_continuation",
            "mmlu_high_school_geography_continuation",
            "mmlu_high_school_government_and_politics_continuation",
            "mmlu_professional_psychology_continuation",
            "mmlu_high_school_macroeconomics_continuation",
            "mmlu_public_relations_continuation",
            "mmlu_security_studies_continuation",
            "mmlu_high_school_microeconomics_continuation",
            "mmlu_sociology_continuation",
            "mmlu_us_foreign_policy_continuation",
            "mmlu_high_school_psychology_continuation",
            "mmlu_human_sexuality_continuation"
        ],
        "other": [
            "mmlu_business_ethics_continuation",
            "mmlu_clinical_knowledge_continuation",
            "mmlu_management_continuation",
            "mmlu_marketing_continuation",
            "mmlu_college_medicine_continuation",
            "mmlu_global_facts_continuation",
            "mmlu_professional_accounting_continuation",
            "mmlu_professional_medicine_continuation",
            "mmlu_virology_continuation",
            "mmlu_human_aging_continuation",
            "mmlu_medical_genetics_continuation",
            "mmlu_miscellaneous_continuation",
            "mmlu_nutrition_continuation"
        ],
        "stem": [
            "mmlu_abstract_algebra_continuation",
            "mmlu_anatomy_continuation",
            "mmlu_astronomy_continuation",
            "mmlu_college_biology_continuation",
            "mmlu_college_chemistry_continuation",
            "mmlu_college_computer_science_continuation",
            "mmlu_college_mathematics_continuation",
            "mmlu_college_physics_continuation",
            "mmlu_computer_security_continuation",
            "mmlu_conceptual_physics_continuation",
            "mmlu_electrical_engineering_continuation",
            "mmlu_elementary_mathematics_continuation",
            "mmlu_high_school_biology_continuation",
            "mmlu_high_school_chemistry_continuation",
            "mmlu_high_school_computer_science_continuation",
            "mmlu_machine_learning_continuation",
            "mmlu_high_school_mathematics_continuation",
            "mmlu_high_school_physics_continuation",
            "mmlu_high_school_statistics_continuation"
        ],
        "mmlu_continuation": [
            "stem",
            "other",
            "social sciences",
            "humanities"
        ],
        "openbookqa": [],
        "piqa": [],
        "winogrande": []
    },
    "configs": {
        "arc_challenge": {
            "task": "arc_challenge",
            "tag": [
                "ai2_arc"
            ],
            "dataset_path": "allenai/ai2_arc",
            "dataset_name": "ARC-Challenge",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{question}}\nAnswer:",
            "doc_to_target": "{{choices.label.index(answerKey)}}",
            "doc_to_choice": "{{choices.text}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "Question: {{question}}\nAnswer:",
            "metadata": {
                "version": 1.0
            }
        },
        "arc_easy": {
            "task": "arc_easy",
            "tag": [
                "ai2_arc"
            ],
            "dataset_path": "allenai/ai2_arc",
            "dataset_name": "ARC-Easy",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{question}}\nAnswer:",
            "doc_to_target": "{{choices.label.index(answerKey)}}",
            "doc_to_choice": "{{choices.text}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "Question: {{question}}\nAnswer:",
            "metadata": {
                "version": 1.0
            }
        },
        "hellaswag": {
            "task": "hellaswag",
            "tag": [
                "multiple_choice"
            ],
            "dataset_path": "Rowan/hellaswag",
            "training_split": "train",
            "validation_split": "validation",
            "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _process_doc(doc):\n        ctx = doc[\"ctx_a\"] + \" \" + doc[\"ctx_b\"].capitalize()\n        out_doc = {\n            \"query\": preprocess(doc[\"activity_label\"] + \": \" + ctx),\n            \"choices\": [preprocess(ending) for ending in doc[\"endings\"]],\n            \"gold\": int(doc[\"label\"]),\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n",
            "doc_to_text": "{{query}}",
            "doc_to_target": "{{label}}",
            "doc_to_choice": "choices",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "lambada_openai": {
            "task": "lambada_openai",
            "tag": [
                "lambada"
            ],
            "dataset_path": "EleutherAI/lambada_openai",
            "dataset_name": "default",
            "test_split": "test",
            "doc_to_text": "{{text.split(' ')[:-1]|join(' ')}}",
            "doc_to_target": "{{' '+text.split(' ')[-1]}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "perplexity",
                    "aggregation": "perplexity",
                    "higher_is_better": false
                },
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "loglikelihood",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "{{text}}",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_abstract_algebra": {
            "task": "mmlu_abstract_algebra",
            "task_alias": "abstract_algebra",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "abstract_algebra",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_abstract_algebra_continuation": {
            "task": "mmlu_abstract_algebra_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "abstract_algebra",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about abstract algebra.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_anatomy": {
            "task": "mmlu_anatomy",
            "task_alias": "anatomy",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "anatomy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_anatomy_continuation": {
            "task": "mmlu_anatomy_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "anatomy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about anatomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_astronomy": {
            "task": "mmlu_astronomy",
            "task_alias": "astronomy",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "astronomy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_astronomy_continuation": {
            "task": "mmlu_astronomy_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "astronomy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about astronomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_business_ethics": {
            "task": "mmlu_business_ethics",
            "task_alias": "business_ethics",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "business_ethics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_business_ethics_continuation": {
            "task": "mmlu_business_ethics_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "business_ethics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about business ethics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_clinical_knowledge": {
            "task": "mmlu_clinical_knowledge",
            "task_alias": "clinical_knowledge",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "clinical_knowledge",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_clinical_knowledge_continuation": {
            "task": "mmlu_clinical_knowledge_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "clinical_knowledge",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about clinical knowledge.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_biology": {
            "task": "mmlu_college_biology",
            "task_alias": "college_biology",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_biology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_biology_continuation": {
            "task": "mmlu_college_biology_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_biology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_chemistry": {
            "task": "mmlu_college_chemistry",
            "task_alias": "college_chemistry",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_chemistry",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_chemistry_continuation": {
            "task": "mmlu_college_chemistry_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_chemistry",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college chemistry.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_computer_science": {
            "task": "mmlu_college_computer_science",
            "task_alias": "college_computer_science",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_computer_science",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_computer_science_continuation": {
            "task": "mmlu_college_computer_science_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_computer_science",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college computer science.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_mathematics": {
            "task": "mmlu_college_mathematics",
            "task_alias": "college_mathematics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_mathematics_continuation": {
            "task": "mmlu_college_mathematics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_medicine": {
            "task": "mmlu_college_medicine",
            "task_alias": "college_medicine",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_medicine",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_medicine_continuation": {
            "task": "mmlu_college_medicine_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_medicine",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_physics": {
            "task": "mmlu_college_physics",
            "task_alias": "college_physics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_physics_continuation": {
            "task": "mmlu_college_physics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_computer_security": {
            "task": "mmlu_computer_security",
            "task_alias": "computer_security",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "computer_security",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_computer_security_continuation": {
            "task": "mmlu_computer_security_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "computer_security",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about computer security.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_conceptual_physics": {
            "task": "mmlu_conceptual_physics",
            "task_alias": "conceptual_physics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "conceptual_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_conceptual_physics_continuation": {
            "task": "mmlu_conceptual_physics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "conceptual_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about conceptual physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_econometrics": {
            "task": "mmlu_econometrics",
            "task_alias": "econometrics",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "econometrics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_econometrics_continuation": {
            "task": "mmlu_econometrics_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "econometrics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about econometrics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_electrical_engineering": {
            "task": "mmlu_electrical_engineering",
            "task_alias": "electrical_engineering",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "electrical_engineering",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_electrical_engineering_continuation": {
            "task": "mmlu_electrical_engineering_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "electrical_engineering",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about electrical engineering.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_elementary_mathematics": {
            "task": "mmlu_elementary_mathematics",
            "task_alias": "elementary_mathematics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "elementary_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_elementary_mathematics_continuation": {
            "task": "mmlu_elementary_mathematics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "elementary_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about elementary mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_formal_logic": {
            "task": "mmlu_formal_logic",
            "task_alias": "formal_logic",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "formal_logic",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_formal_logic_continuation": {
            "task": "mmlu_formal_logic_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "formal_logic",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about formal logic.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_global_facts": {
            "task": "mmlu_global_facts",
            "task_alias": "global_facts",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "global_facts",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_global_facts_continuation": {
            "task": "mmlu_global_facts_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "global_facts",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about global facts.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_biology": {
            "task": "mmlu_high_school_biology",
            "task_alias": "high_school_biology",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_biology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_biology_continuation": {
            "task": "mmlu_high_school_biology_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_biology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_chemistry": {
            "task": "mmlu_high_school_chemistry",
            "task_alias": "high_school_chemistry",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_chemistry",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_chemistry_continuation": {
            "task": "mmlu_high_school_chemistry_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_chemistry",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school chemistry.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_computer_science": {
            "task": "mmlu_high_school_computer_science",
            "task_alias": "high_school_computer_science",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_computer_science",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_computer_science_continuation": {
            "task": "mmlu_high_school_computer_science_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_computer_science",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school computer science.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_european_history": {
            "task": "mmlu_high_school_european_history",
            "task_alias": "high_school_european_history",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_european_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_european_history_continuation": {
            "task": "mmlu_high_school_european_history_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_european_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school european history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_geography": {
            "task": "mmlu_high_school_geography",
            "task_alias": "high_school_geography",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_geography",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_geography_continuation": {
            "task": "mmlu_high_school_geography_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_geography",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school geography.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_government_and_politics": {
            "task": "mmlu_high_school_government_and_politics",
            "task_alias": "high_school_government_and_politics",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_government_and_politics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_government_and_politics_continuation": {
            "task": "mmlu_high_school_government_and_politics_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_government_and_politics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school government and politics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_macroeconomics": {
            "task": "mmlu_high_school_macroeconomics",
            "task_alias": "high_school_macroeconomics",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_macroeconomics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_macroeconomics_continuation": {
            "task": "mmlu_high_school_macroeconomics_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_macroeconomics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school macroeconomics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_mathematics": {
            "task": "mmlu_high_school_mathematics",
            "task_alias": "high_school_mathematics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_mathematics_continuation": {
            "task": "mmlu_high_school_mathematics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_microeconomics": {
            "task": "mmlu_high_school_microeconomics",
            "task_alias": "high_school_microeconomics",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_microeconomics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_microeconomics_continuation": {
            "task": "mmlu_high_school_microeconomics_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_microeconomics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school microeconomics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_physics": {
            "task": "mmlu_high_school_physics",
            "task_alias": "high_school_physics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_physics_continuation": {
            "task": "mmlu_high_school_physics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_psychology": {
            "task": "mmlu_high_school_psychology",
            "task_alias": "high_school_psychology",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_psychology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_psychology_continuation": {
            "task": "mmlu_high_school_psychology_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_psychology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school psychology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_statistics": {
            "task": "mmlu_high_school_statistics",
            "task_alias": "high_school_statistics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_statistics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_statistics_continuation": {
            "task": "mmlu_high_school_statistics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_statistics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school statistics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_us_history": {
            "task": "mmlu_high_school_us_history",
            "task_alias": "high_school_us_history",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_us_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_us_history_continuation": {
            "task": "mmlu_high_school_us_history_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_us_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school us history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_world_history": {
            "task": "mmlu_high_school_world_history",
            "task_alias": "high_school_world_history",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_world_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_world_history_continuation": {
            "task": "mmlu_high_school_world_history_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_world_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school world history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_human_aging": {
            "task": "mmlu_human_aging",
            "task_alias": "human_aging",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "human_aging",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_human_aging_continuation": {
            "task": "mmlu_human_aging_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "human_aging",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about human aging.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_human_sexuality": {
            "task": "mmlu_human_sexuality",
            "task_alias": "human_sexuality",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "human_sexuality",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_human_sexuality_continuation": {
            "task": "mmlu_human_sexuality_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "human_sexuality",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about human sexuality.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_international_law": {
            "task": "mmlu_international_law",
            "task_alias": "international_law",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "international_law",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about international law.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_international_law_continuation": {
            "task": "mmlu_international_law_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "international_law",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about international law.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_jurisprudence": {
            "task": "mmlu_jurisprudence",
            "task_alias": "jurisprudence",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "jurisprudence",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_jurisprudence_continuation": {
            "task": "mmlu_jurisprudence_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "jurisprudence",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about jurisprudence.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_logical_fallacies": {
            "task": "mmlu_logical_fallacies",
            "task_alias": "logical_fallacies",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "logical_fallacies",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_logical_fallacies_continuation": {
            "task": "mmlu_logical_fallacies_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "logical_fallacies",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about logical fallacies.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_machine_learning": {
            "task": "mmlu_machine_learning",
            "task_alias": "machine_learning",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "machine_learning",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_machine_learning_continuation": {
            "task": "mmlu_machine_learning_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "machine_learning",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about machine learning.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_management": {
            "task": "mmlu_management",
            "task_alias": "management",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "management",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about management.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_management_continuation": {
            "task": "mmlu_management_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "management",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about management.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_marketing": {
            "task": "mmlu_marketing",
            "task_alias": "marketing",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "marketing",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_marketing_continuation": {
            "task": "mmlu_marketing_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "marketing",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about marketing.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_medical_genetics": {
            "task": "mmlu_medical_genetics",
            "task_alias": "medical_genetics",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "medical_genetics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_medical_genetics_continuation": {
            "task": "mmlu_medical_genetics_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "medical_genetics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about medical genetics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_miscellaneous": {
            "task": "mmlu_miscellaneous",
            "task_alias": "miscellaneous",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "miscellaneous",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_miscellaneous_continuation": {
            "task": "mmlu_miscellaneous_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "miscellaneous",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about miscellaneous.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_moral_disputes": {
            "task": "mmlu_moral_disputes",
            "task_alias": "moral_disputes",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "moral_disputes",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_moral_disputes_continuation": {
            "task": "mmlu_moral_disputes_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "moral_disputes",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about moral disputes.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_moral_scenarios": {
            "task": "mmlu_moral_scenarios",
            "task_alias": "moral_scenarios",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "moral_scenarios",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_moral_scenarios_continuation": {
            "task": "mmlu_moral_scenarios_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "moral_scenarios",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about moral scenarios.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_nutrition": {
            "task": "mmlu_nutrition",
            "task_alias": "nutrition",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "nutrition",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_nutrition_continuation": {
            "task": "mmlu_nutrition_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "nutrition",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about nutrition.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_philosophy": {
            "task": "mmlu_philosophy",
            "task_alias": "philosophy",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "philosophy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_philosophy_continuation": {
            "task": "mmlu_philosophy_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "philosophy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about philosophy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_prehistory": {
            "task": "mmlu_prehistory",
            "task_alias": "prehistory",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "prehistory",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_prehistory_continuation": {
            "task": "mmlu_prehistory_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "prehistory",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about prehistory.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_accounting": {
            "task": "mmlu_professional_accounting",
            "task_alias": "professional_accounting",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_accounting",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_accounting_continuation": {
            "task": "mmlu_professional_accounting_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_accounting",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional accounting.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_law": {
            "task": "mmlu_professional_law",
            "task_alias": "professional_law",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_law",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_law_continuation": {
            "task": "mmlu_professional_law_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_law",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional law.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_medicine": {
            "task": "mmlu_professional_medicine",
            "task_alias": "professional_medicine",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_medicine",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_medicine_continuation": {
            "task": "mmlu_professional_medicine_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_medicine",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_psychology": {
            "task": "mmlu_professional_psychology",
            "task_alias": "professional_psychology",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_psychology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_psychology_continuation": {
            "task": "mmlu_professional_psychology_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_psychology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional psychology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_public_relations": {
            "task": "mmlu_public_relations",
            "task_alias": "public_relations",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "public_relations",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_public_relations_continuation": {
            "task": "mmlu_public_relations_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "public_relations",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about public relations.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_security_studies": {
            "task": "mmlu_security_studies",
            "task_alias": "security_studies",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "security_studies",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_security_studies_continuation": {
            "task": "mmlu_security_studies_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "security_studies",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about security studies.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_sociology": {
            "task": "mmlu_sociology",
            "task_alias": "sociology",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "sociology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_sociology_continuation": {
            "task": "mmlu_sociology_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "sociology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about sociology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_us_foreign_policy": {
            "task": "mmlu_us_foreign_policy",
            "task_alias": "us_foreign_policy",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "us_foreign_policy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_us_foreign_policy_continuation": {
            "task": "mmlu_us_foreign_policy_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "us_foreign_policy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about us foreign policy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_virology": {
            "task": "mmlu_virology",
            "task_alias": "virology",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "virology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about virology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_virology_continuation": {
            "task": "mmlu_virology_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "virology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about virology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_world_religions": {
            "task": "mmlu_world_religions",
            "task_alias": "world_religions",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "world_religions",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_world_religions_continuation": {
            "task": "mmlu_world_religions_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "world_religions",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about world religions.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "openbookqa": {
            "task": "openbookqa",
            "dataset_path": "openbookqa",
            "dataset_name": "main",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "question_stem",
            "doc_to_target": "{{choices.label.index(answerKey.lstrip())}}",
            "doc_to_choice": "{{choices.text}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "question_stem",
            "metadata": {
                "version": 1.0
            }
        },
        "piqa": {
            "task": "piqa",
            "dataset_path": "baber/piqa",
            "training_split": "train",
            "validation_split": "validation",
            "doc_to_text": "Question: {{goal}}\nAnswer:",
            "doc_to_target": "label",
            "doc_to_choice": "{{[sol1, sol2]}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "goal",
            "metadata": {
                "version": 1.0
            }
        },
        "winogrande": {
            "task": "winogrande",
            "dataset_path": "winogrande",
            "dataset_name": "winogrande_xl",
            "training_split": "train",
            "validation_split": "validation",
            "doc_to_text": "def doc_to_text(doc):\n    answer_to_num = {\"1\": 0, \"2\": 1}\n    return answer_to_num[doc[\"answer\"]]\n",
            "doc_to_target": "def doc_to_target(doc):\n    idx = doc[\"sentence\"].index(\"_\") + 1\n    return doc[\"sentence\"][idx:].strip()\n",
            "doc_to_choice": "def doc_to_choice(doc):\n    idx = doc[\"sentence\"].index(\"_\")\n    options = [doc[\"option1\"], doc[\"option2\"]]\n    return [doc[\"sentence\"][:idx] + opt for opt in options]\n",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "sentence",
            "metadata": {
                "version": 1.0
            }
        }
    },
    "versions": {
        "arc_challenge": 1.0,
        "arc_easy": 1.0,
        "hellaswag": 1.0,
        "lambada_openai": 1.0,
        "mmlu": 2,
        "mmlu_abstract_algebra": 1.0,
        "mmlu_abstract_algebra_continuation": 1.0,
        "mmlu_anatomy": 1.0,
        "mmlu_anatomy_continuation": 1.0,
        "mmlu_astronomy": 1.0,
        "mmlu_astronomy_continuation": 1.0,
        "mmlu_business_ethics": 1.0,
        "mmlu_business_ethics_continuation": 1.0,
        "mmlu_clinical_knowledge": 1.0,
        "mmlu_clinical_knowledge_continuation": 1.0,
        "mmlu_college_biology": 1.0,
        "mmlu_college_biology_continuation": 1.0,
        "mmlu_college_chemistry": 1.0,
        "mmlu_college_chemistry_continuation": 1.0,
        "mmlu_college_computer_science": 1.0,
        "mmlu_college_computer_science_continuation": 1.0,
        "mmlu_college_mathematics": 1.0,
        "mmlu_college_mathematics_continuation": 1.0,
        "mmlu_college_medicine": 1.0,
        "mmlu_college_medicine_continuation": 1.0,
        "mmlu_college_physics": 1.0,
        "mmlu_college_physics_continuation": 1.0,
        "mmlu_computer_security": 1.0,
        "mmlu_computer_security_continuation": 1.0,
        "mmlu_conceptual_physics": 1.0,
        "mmlu_conceptual_physics_continuation": 1.0,
        "mmlu_continuation": 2,
        "mmlu_econometrics": 1.0,
        "mmlu_econometrics_continuation": 1.0,
        "mmlu_electrical_engineering": 1.0,
        "mmlu_electrical_engineering_continuation": 1.0,
        "mmlu_elementary_mathematics": 1.0,
        "mmlu_elementary_mathematics_continuation": 1.0,
        "mmlu_formal_logic": 1.0,
        "mmlu_formal_logic_continuation": 1.0,
        "mmlu_global_facts": 1.0,
        "mmlu_global_facts_continuation": 1.0,
        "mmlu_high_school_biology": 1.0,
        "mmlu_high_school_biology_continuation": 1.0,
        "mmlu_high_school_chemistry": 1.0,
        "mmlu_high_school_chemistry_continuation": 1.0,
        "mmlu_high_school_computer_science": 1.0,
        "mmlu_high_school_computer_science_continuation": 1.0,
        "mmlu_high_school_european_history": 1.0,
        "mmlu_high_school_european_history_continuation": 1.0,
        "mmlu_high_school_geography": 1.0,
        "mmlu_high_school_geography_continuation": 1.0,
        "mmlu_high_school_government_and_politics": 1.0,
        "mmlu_high_school_government_and_politics_continuation": 1.0,
        "mmlu_high_school_macroeconomics": 1.0,
        "mmlu_high_school_macroeconomics_continuation": 1.0,
        "mmlu_high_school_mathematics": 1.0,
        "mmlu_high_school_mathematics_continuation": 1.0,
        "mmlu_high_school_microeconomics": 1.0,
        "mmlu_high_school_microeconomics_continuation": 1.0,
        "mmlu_high_school_physics": 1.0,
        "mmlu_high_school_physics_continuation": 1.0,
        "mmlu_high_school_psychology": 1.0,
        "mmlu_high_school_psychology_continuation": 1.0,
        "mmlu_high_school_statistics": 1.0,
        "mmlu_high_school_statistics_continuation": 1.0,
        "mmlu_high_school_us_history": 1.0,
        "mmlu_high_school_us_history_continuation": 1.0,
        "mmlu_high_school_world_history": 1.0,
        "mmlu_high_school_world_history_continuation": 1.0,
        "mmlu_human_aging": 1.0,
        "mmlu_human_aging_continuation": 1.0,
        "mmlu_human_sexuality": 1.0,
        "mmlu_human_sexuality_continuation": 1.0,
        "mmlu_humanities": 2,
        "mmlu_international_law": 1.0,
        "mmlu_international_law_continuation": 1.0,
        "mmlu_jurisprudence": 1.0,
        "mmlu_jurisprudence_continuation": 1.0,
        "mmlu_logical_fallacies": 1.0,
        "mmlu_logical_fallacies_continuation": 1.0,
        "mmlu_machine_learning": 1.0,
        "mmlu_machine_learning_continuation": 1.0,
        "mmlu_management": 1.0,
        "mmlu_management_continuation": 1.0,
        "mmlu_marketing": 1.0,
        "mmlu_marketing_continuation": 1.0,
        "mmlu_medical_genetics": 1.0,
        "mmlu_medical_genetics_continuation": 1.0,
        "mmlu_miscellaneous": 1.0,
        "mmlu_miscellaneous_continuation": 1.0,
        "mmlu_moral_disputes": 1.0,
        "mmlu_moral_disputes_continuation": 1.0,
        "mmlu_moral_scenarios": 1.0,
        "mmlu_moral_scenarios_continuation": 1.0,
        "mmlu_nutrition": 1.0,
        "mmlu_nutrition_continuation": 1.0,
        "mmlu_other": 2,
        "mmlu_philosophy": 1.0,
        "mmlu_philosophy_continuation": 1.0,
        "mmlu_prehistory": 1.0,
        "mmlu_prehistory_continuation": 1.0,
        "mmlu_professional_accounting": 1.0,
        "mmlu_professional_accounting_continuation": 1.0,
        "mmlu_professional_law": 1.0,
        "mmlu_professional_law_continuation": 1.0,
        "mmlu_professional_medicine": 1.0,
        "mmlu_professional_medicine_continuation": 1.0,
        "mmlu_professional_psychology": 1.0,
        "mmlu_professional_psychology_continuation": 1.0,
        "mmlu_public_relations": 1.0,
        "mmlu_public_relations_continuation": 1.0,
        "mmlu_security_studies": 1.0,
        "mmlu_security_studies_continuation": 1.0,
        "mmlu_social_sciences": 2,
        "mmlu_sociology": 1.0,
        "mmlu_sociology_continuation": 1.0,
        "mmlu_stem": 2,
        "mmlu_us_foreign_policy": 1.0,
        "mmlu_us_foreign_policy_continuation": 1.0,
        "mmlu_virology": 1.0,
        "mmlu_virology_continuation": 1.0,
        "mmlu_world_religions": 1.0,
        "mmlu_world_religions_continuation": 1.0,
        "openbookqa": 1.0,
        "piqa": 1.0,
        "winogrande": 1.0
    },
    "n-shot": {
        "arc_challenge": 0,
        "arc_easy": 0,
        "hellaswag": 0,
        "lambada_openai": 0,
        "mmlu_abstract_algebra": 0,
        "mmlu_abstract_algebra_continuation": 0,
        "mmlu_anatomy": 0,
        "mmlu_anatomy_continuation": 0,
        "mmlu_astronomy": 0,
        "mmlu_astronomy_continuation": 0,
        "mmlu_business_ethics": 0,
        "mmlu_business_ethics_continuation": 0,
        "mmlu_clinical_knowledge": 0,
        "mmlu_clinical_knowledge_continuation": 0,
        "mmlu_college_biology": 0,
        "mmlu_college_biology_continuation": 0,
        "mmlu_college_chemistry": 0,
        "mmlu_college_chemistry_continuation": 0,
        "mmlu_college_computer_science": 0,
        "mmlu_college_computer_science_continuation": 0,
        "mmlu_college_mathematics": 0,
        "mmlu_college_mathematics_continuation": 0,
        "mmlu_college_medicine": 0,
        "mmlu_college_medicine_continuation": 0,
        "mmlu_college_physics": 0,
        "mmlu_college_physics_continuation": 0,
        "mmlu_computer_security": 0,
        "mmlu_computer_security_continuation": 0,
        "mmlu_conceptual_physics": 0,
        "mmlu_conceptual_physics_continuation": 0,
        "mmlu_econometrics": 0,
        "mmlu_econometrics_continuation": 0,
        "mmlu_electrical_engineering": 0,
        "mmlu_electrical_engineering_continuation": 0,
        "mmlu_elementary_mathematics": 0,
        "mmlu_elementary_mathematics_continuation": 0,
        "mmlu_formal_logic": 0,
        "mmlu_formal_logic_continuation": 0,
        "mmlu_global_facts": 0,
        "mmlu_global_facts_continuation": 0,
        "mmlu_high_school_biology": 0,
        "mmlu_high_school_biology_continuation": 0,
        "mmlu_high_school_chemistry": 0,
        "mmlu_high_school_chemistry_continuation": 0,
        "mmlu_high_school_computer_science": 0,
        "mmlu_high_school_computer_science_continuation": 0,
        "mmlu_high_school_european_history": 0,
        "mmlu_high_school_european_history_continuation": 0,
        "mmlu_high_school_geography": 0,
        "mmlu_high_school_geography_continuation": 0,
        "mmlu_high_school_government_and_politics": 0,
        "mmlu_high_school_government_and_politics_continuation": 0,
        "mmlu_high_school_macroeconomics": 0,
        "mmlu_high_school_macroeconomics_continuation": 0,
        "mmlu_high_school_mathematics": 0,
        "mmlu_high_school_mathematics_continuation": 0,
        "mmlu_high_school_microeconomics": 0,
        "mmlu_high_school_microeconomics_continuation": 0,
        "mmlu_high_school_physics": 0,
        "mmlu_high_school_physics_continuation": 0,
        "mmlu_high_school_psychology": 0,
        "mmlu_high_school_psychology_continuation": 0,
        "mmlu_high_school_statistics": 0,
        "mmlu_high_school_statistics_continuation": 0,
        "mmlu_high_school_us_history": 0,
        "mmlu_high_school_us_history_continuation": 0,
        "mmlu_high_school_world_history": 0,
        "mmlu_high_school_world_history_continuation": 0,
        "mmlu_human_aging": 0,
        "mmlu_human_aging_continuation": 0,
        "mmlu_human_sexuality": 0,
        "mmlu_human_sexuality_continuation": 0,
        "mmlu_international_law": 0,
        "mmlu_international_law_continuation": 0,
        "mmlu_jurisprudence": 0,
        "mmlu_jurisprudence_continuation": 0,
        "mmlu_logical_fallacies": 0,
        "mmlu_logical_fallacies_continuation": 0,
        "mmlu_machine_learning": 0,
        "mmlu_machine_learning_continuation": 0,
        "mmlu_management": 0,
        "mmlu_management_continuation": 0,
        "mmlu_marketing": 0,
        "mmlu_marketing_continuation": 0,
        "mmlu_medical_genetics": 0,
        "mmlu_medical_genetics_continuation": 0,
        "mmlu_miscellaneous": 0,
        "mmlu_miscellaneous_continuation": 0,
        "mmlu_moral_disputes": 0,
        "mmlu_moral_disputes_continuation": 0,
        "mmlu_moral_scenarios": 0,
        "mmlu_moral_scenarios_continuation": 0,
        "mmlu_nutrition": 0,
        "mmlu_nutrition_continuation": 0,
        "mmlu_philosophy": 0,
        "mmlu_philosophy_continuation": 0,
        "mmlu_prehistory": 0,
        "mmlu_prehistory_continuation": 0,
        "mmlu_professional_accounting": 0,
        "mmlu_professional_accounting_continuation": 0,
        "mmlu_professional_law": 0,
        "mmlu_professional_law_continuation": 0,
        "mmlu_professional_medicine": 0,
        "mmlu_professional_medicine_continuation": 0,
        "mmlu_professional_psychology": 0,
        "mmlu_professional_psychology_continuation": 0,
        "mmlu_public_relations": 0,
        "mmlu_public_relations_continuation": 0,
        "mmlu_security_studies": 0,
        "mmlu_security_studies_continuation": 0,
        "mmlu_sociology": 0,
        "mmlu_sociology_continuation": 0,
        "mmlu_us_foreign_policy": 0,
        "mmlu_us_foreign_policy_continuation": 0,
        "mmlu_virology": 0,
        "mmlu_virology_continuation": 0,
        "mmlu_world_religions": 0,
        "mmlu_world_religions_continuation": 0,
        "openbookqa": 0,
        "piqa": 0,
        "winogrande": 0
    },
    "higher_is_better": {
        "arc_challenge": {
            "acc": true,
            "acc_norm": true
        },
        "arc_easy": {
            "acc": true,
            "acc_norm": true
        },
        "hellaswag": {
            "acc": true,
            "acc_norm": true
        },
        "humanities": {
            "acc": true,
            "acc_norm": true
        },
        "lambada_openai": {
            "perplexity": false,
            "acc": true
        },
        "mmlu": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_abstract_algebra": {
            "acc": true
        },
        "mmlu_abstract_algebra_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_anatomy": {
            "acc": true
        },
        "mmlu_anatomy_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_astronomy": {
            "acc": true
        },
        "mmlu_astronomy_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_business_ethics": {
            "acc": true
        },
        "mmlu_business_ethics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_clinical_knowledge": {
            "acc": true
        },
        "mmlu_clinical_knowledge_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_biology": {
            "acc": true
        },
        "mmlu_college_biology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_chemistry": {
            "acc": true
        },
        "mmlu_college_chemistry_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_computer_science": {
            "acc": true
        },
        "mmlu_college_computer_science_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_mathematics": {
            "acc": true
        },
        "mmlu_college_mathematics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_medicine": {
            "acc": true
        },
        "mmlu_college_medicine_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_physics": {
            "acc": true
        },
        "mmlu_college_physics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_computer_security": {
            "acc": true
        },
        "mmlu_computer_security_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_conceptual_physics": {
            "acc": true
        },
        "mmlu_conceptual_physics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_econometrics": {
            "acc": true
        },
        "mmlu_econometrics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_electrical_engineering": {
            "acc": true
        },
        "mmlu_electrical_engineering_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_elementary_mathematics": {
            "acc": true
        },
        "mmlu_elementary_mathematics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_formal_logic": {
            "acc": true
        },
        "mmlu_formal_logic_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_global_facts": {
            "acc": true
        },
        "mmlu_global_facts_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_biology": {
            "acc": true
        },
        "mmlu_high_school_biology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_chemistry": {
            "acc": true
        },
        "mmlu_high_school_chemistry_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_computer_science": {
            "acc": true
        },
        "mmlu_high_school_computer_science_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_european_history": {
            "acc": true
        },
        "mmlu_high_school_european_history_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_geography": {
            "acc": true
        },
        "mmlu_high_school_geography_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_government_and_politics": {
            "acc": true
        },
        "mmlu_high_school_government_and_politics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_macroeconomics": {
            "acc": true
        },
        "mmlu_high_school_macroeconomics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_mathematics": {
            "acc": true
        },
        "mmlu_high_school_mathematics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_microeconomics": {
            "acc": true
        },
        "mmlu_high_school_microeconomics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_physics": {
            "acc": true
        },
        "mmlu_high_school_physics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_psychology": {
            "acc": true
        },
        "mmlu_high_school_psychology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_statistics": {
            "acc": true
        },
        "mmlu_high_school_statistics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_us_history": {
            "acc": true
        },
        "mmlu_high_school_us_history_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_world_history": {
            "acc": true
        },
        "mmlu_high_school_world_history_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_human_aging": {
            "acc": true
        },
        "mmlu_human_aging_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_human_sexuality": {
            "acc": true
        },
        "mmlu_human_sexuality_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_humanities": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_international_law": {
            "acc": true
        },
        "mmlu_international_law_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_jurisprudence": {
            "acc": true
        },
        "mmlu_jurisprudence_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_logical_fallacies": {
            "acc": true
        },
        "mmlu_logical_fallacies_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_machine_learning": {
            "acc": true
        },
        "mmlu_machine_learning_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_management": {
            "acc": true
        },
        "mmlu_management_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_marketing": {
            "acc": true
        },
        "mmlu_marketing_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_medical_genetics": {
            "acc": true
        },
        "mmlu_medical_genetics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_miscellaneous": {
            "acc": true
        },
        "mmlu_miscellaneous_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_moral_disputes": {
            "acc": true
        },
        "mmlu_moral_disputes_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_moral_scenarios": {
            "acc": true
        },
        "mmlu_moral_scenarios_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_nutrition": {
            "acc": true
        },
        "mmlu_nutrition_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_other": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_philosophy": {
            "acc": true
        },
        "mmlu_philosophy_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_prehistory": {
            "acc": true
        },
        "mmlu_prehistory_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_professional_accounting": {
            "acc": true
        },
        "mmlu_professional_accounting_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_professional_law": {
            "acc": true
        },
        "mmlu_professional_law_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_professional_medicine": {
            "acc": true
        },
        "mmlu_professional_medicine_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_professional_psychology": {
            "acc": true
        },
        "mmlu_professional_psychology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_public_relations": {
            "acc": true
        },
        "mmlu_public_relations_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_security_studies": {
            "acc": true
        },
        "mmlu_security_studies_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_social_sciences": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_sociology": {
            "acc": true
        },
        "mmlu_sociology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_stem": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_us_foreign_policy": {
            "acc": true
        },
        "mmlu_us_foreign_policy_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_virology": {
            "acc": true
        },
        "mmlu_virology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_world_religions": {
            "acc": true
        },
        "mmlu_world_religions_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "openbookqa": {
            "acc": true,
            "acc_norm": true
        },
        "other": {
            "acc": true,
            "acc_norm": true
        },
        "piqa": {
            "acc": true,
            "acc_norm": true
        },
        "social sciences": {
            "acc": true,
            "acc_norm": true
        },
        "stem": {
            "acc": true,
            "acc_norm": true
        },
        "winogrande": {
            "acc": true
        }
    },
    "n-samples": {
        "winogrande": {
            "original": 1267,
            "effective": 1267
        },
        "piqa": {
            "original": 1838,
            "effective": 1838
        },
        "openbookqa": {
            "original": 500,
            "effective": 500
        },
        "mmlu_abstract_algebra_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_anatomy_continuation": {
            "original": 135,
            "effective": 135
        },
        "mmlu_astronomy_continuation": {
            "original": 152,
            "effective": 152
        },
        "mmlu_college_biology_continuation": {
            "original": 144,
            "effective": 144
        },
        "mmlu_college_chemistry_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_computer_science_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_mathematics_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_physics_continuation": {
            "original": 102,
            "effective": 102
        },
        "mmlu_computer_security_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_conceptual_physics_continuation": {
            "original": 235,
            "effective": 235
        },
        "mmlu_electrical_engineering_continuation": {
            "original": 145,
            "effective": 145
        },
        "mmlu_elementary_mathematics_continuation": {
            "original": 378,
            "effective": 378
        },
        "mmlu_high_school_biology_continuation": {
            "original": 310,
            "effective": 310
        },
        "mmlu_high_school_chemistry_continuation": {
            "original": 203,
            "effective": 203
        },
        "mmlu_high_school_computer_science_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_machine_learning_continuation": {
            "original": 112,
            "effective": 112
        },
        "mmlu_high_school_mathematics_continuation": {
            "original": 270,
            "effective": 270
        },
        "mmlu_high_school_physics_continuation": {
            "original": 151,
            "effective": 151
        },
        "mmlu_high_school_statistics_continuation": {
            "original": 216,
            "effective": 216
        },
        "mmlu_business_ethics_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_clinical_knowledge_continuation": {
            "original": 265,
            "effective": 265
        },
        "mmlu_management_continuation": {
            "original": 103,
            "effective": 103
        },
        "mmlu_marketing_continuation": {
            "original": 234,
            "effective": 234
        },
        "mmlu_college_medicine_continuation": {
            "original": 173,
            "effective": 173
        },
        "mmlu_global_facts_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_professional_accounting_continuation": {
            "original": 282,
            "effective": 282
        },
        "mmlu_professional_medicine_continuation": {
            "original": 272,
            "effective": 272
        },
        "mmlu_virology_continuation": {
            "original": 166,
            "effective": 166
        },
        "mmlu_human_aging_continuation": {
            "original": 223,
            "effective": 223
        },
        "mmlu_medical_genetics_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_miscellaneous_continuation": {
            "original": 783,
            "effective": 783
        },
        "mmlu_nutrition_continuation": {
            "original": 306,
            "effective": 306
        },
        "mmlu_econometrics_continuation": {
            "original": 114,
            "effective": 114
        },
        "mmlu_high_school_geography_continuation": {
            "original": 198,
            "effective": 198
        },
        "mmlu_high_school_government_and_politics_continuation": {
            "original": 193,
            "effective": 193
        },
        "mmlu_professional_psychology_continuation": {
            "original": 612,
            "effective": 612
        },
        "mmlu_high_school_macroeconomics_continuation": {
            "original": 390,
            "effective": 390
        },
        "mmlu_public_relations_continuation": {
            "original": 110,
            "effective": 110
        },
        "mmlu_security_studies_continuation": {
            "original": 245,
            "effective": 245
        },
        "mmlu_high_school_microeconomics_continuation": {
            "original": 238,
            "effective": 238
        },
        "mmlu_sociology_continuation": {
            "original": 201,
            "effective": 201
        },
        "mmlu_us_foreign_policy_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_high_school_psychology_continuation": {
            "original": 545,
            "effective": 545
        },
        "mmlu_human_sexuality_continuation": {
            "original": 131,
            "effective": 131
        },
        "mmlu_philosophy_continuation": {
            "original": 311,
            "effective": 311
        },
        "mmlu_prehistory_continuation": {
            "original": 324,
            "effective": 324
        },
        "mmlu_formal_logic_continuation": {
            "original": 126,
            "effective": 126
        },
        "mmlu_professional_law_continuation": {
            "original": 1534,
            "effective": 1534
        },
        "mmlu_logical_fallacies_continuation": {
            "original": 163,
            "effective": 163
        },
        "mmlu_high_school_european_history_continuation": {
            "original": 165,
            "effective": 165
        },
        "mmlu_world_religions_continuation": {
            "original": 171,
            "effective": 171
        },
        "mmlu_high_school_us_history_continuation": {
            "original": 204,
            "effective": 204
        },
        "mmlu_high_school_world_history_continuation": {
            "original": 237,
            "effective": 237
        },
        "mmlu_international_law_continuation": {
            "original": 121,
            "effective": 121
        },
        "mmlu_jurisprudence_continuation": {
            "original": 108,
            "effective": 108
        },
        "mmlu_moral_disputes_continuation": {
            "original": 346,
            "effective": 346
        },
        "mmlu_moral_scenarios_continuation": {
            "original": 895,
            "effective": 895
        },
        "mmlu_abstract_algebra": {
            "original": 100,
            "effective": 100
        },
        "mmlu_anatomy": {
            "original": 135,
            "effective": 135
        },
        "mmlu_astronomy": {
            "original": 152,
            "effective": 152
        },
        "mmlu_college_biology": {
            "original": 144,
            "effective": 144
        },
        "mmlu_college_chemistry": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_computer_science": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_mathematics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_physics": {
            "original": 102,
            "effective": 102
        },
        "mmlu_computer_security": {
            "original": 100,
            "effective": 100
        },
        "mmlu_conceptual_physics": {
            "original": 235,
            "effective": 235
        },
        "mmlu_electrical_engineering": {
            "original": 145,
            "effective": 145
        },
        "mmlu_machine_learning": {
            "original": 112,
            "effective": 112
        },
        "mmlu_elementary_mathematics": {
            "original": 378,
            "effective": 378
        },
        "mmlu_high_school_biology": {
            "original": 310,
            "effective": 310
        },
        "mmlu_high_school_chemistry": {
            "original": 203,
            "effective": 203
        },
        "mmlu_high_school_computer_science": {
            "original": 100,
            "effective": 100
        },
        "mmlu_high_school_mathematics": {
            "original": 270,
            "effective": 270
        },
        "mmlu_high_school_physics": {
            "original": 151,
            "effective": 151
        },
        "mmlu_high_school_statistics": {
            "original": 216,
            "effective": 216
        },
        "mmlu_business_ethics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_clinical_knowledge": {
            "original": 265,
            "effective": 265
        },
        "mmlu_college_medicine": {
            "original": 173,
            "effective": 173
        },
        "mmlu_global_facts": {
            "original": 100,
            "effective": 100
        },
        "mmlu_management": {
            "original": 103,
            "effective": 103
        },
        "mmlu_marketing": {
            "original": 234,
            "effective": 234
        },
        "mmlu_human_aging": {
            "original": 223,
            "effective": 223
        },
        "mmlu_nutrition": {
            "original": 306,
            "effective": 306
        },
        "mmlu_professional_accounting": {
            "original": 282,
            "effective": 282
        },
        "mmlu_professional_medicine": {
            "original": 272,
            "effective": 272
        },
        "mmlu_medical_genetics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_miscellaneous": {
            "original": 783,
            "effective": 783
        },
        "mmlu_virology": {
            "original": 166,
            "effective": 166
        },
        "mmlu_us_foreign_policy": {
            "original": 100,
            "effective": 100
        },
        "mmlu_econometrics": {
            "original": 114,
            "effective": 114
        },
        "mmlu_human_sexuality": {
            "original": 131,
            "effective": 131
        },
        "mmlu_high_school_geography": {
            "original": 198,
            "effective": 198
        },
        "mmlu_high_school_government_and_politics": {
            "original": 193,
            "effective": 193
        },
        "mmlu_high_school_macroeconomics": {
            "original": 390,
            "effective": 390
        },
        "mmlu_high_school_microeconomics": {
            "original": 238,
            "effective": 238
        },
        "mmlu_professional_psychology": {
            "original": 612,
            "effective": 612
        },
        "mmlu_high_school_psychology": {
            "original": 545,
            "effective": 545
        },
        "mmlu_public_relations": {
            "original": 110,
            "effective": 110
        },
        "mmlu_security_studies": {
            "original": 245,
            "effective": 245
        },
        "mmlu_sociology": {
            "original": 201,
            "effective": 201
        },
        "mmlu_world_religions": {
            "original": 171,
            "effective": 171
        },
        "mmlu_international_law": {
            "original": 121,
            "effective": 121
        },
        "mmlu_jurisprudence": {
            "original": 108,
            "effective": 108
        },
        "mmlu_logical_fallacies": {
            "original": 163,
            "effective": 163
        },
        "mmlu_formal_logic": {
            "original": 126,
            "effective": 126
        },
        "mmlu_high_school_european_history": {
            "original": 165,
            "effective": 165
        },
        "mmlu_philosophy": {
            "original": 311,
            "effective": 311
        },
        "mmlu_prehistory": {
            "original": 324,
            "effective": 324
        },
        "mmlu_professional_law": {
            "original": 1534,
            "effective": 1534
        },
        "mmlu_high_school_us_history": {
            "original": 204,
            "effective": 204
        },
        "mmlu_high_school_world_history": {
            "original": 237,
            "effective": 237
        },
        "mmlu_moral_disputes": {
            "original": 346,
            "effective": 346
        },
        "mmlu_moral_scenarios": {
            "original": 895,
            "effective": 895
        },
        "lambada_openai": {
            "original": 5153,
            "effective": 5153
        },
        "hellaswag": {
            "original": 10042,
            "effective": 10042
        },
        "arc_easy": {
            "original": 2376,
            "effective": 2376
        },
        "arc_challenge": {
            "original": 1172,
            "effective": 1172
        }
    },
    "config": {
        "model": "/root/mixture_of_recursions/hf_models/SmolLM-360M",
        "model_args": "pretrained=/root/mixture_of_recursions/results/pretrain/250823_pretrain_smollm-360m_rec2_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001,trust_remote_code=True",
        "model_num_parameters": 134684932,
        "model_dtype": "torch.bfloat16",
        "model_revision": "main",
        "model_sha": "",
        "batch_size": 32,
        "batch_sizes": [],
        "device": "cuda:0",
        "use_cache": null,
        "limit": null,
        "bootstrap_iters": 100000,
        "gen_kwargs": null,
        "random_seed": 0,
        "numpy_seed": 1234,
        "torch_seed": 1234,
        "fewshot_seed": 1234
    },
    "git_hash": "d2fd900",
    "date": 1756201583.908796,
    "pretty_env_info": "PyTorch version: 2.8.0+cu128\nIs debug build: False\nCUDA used to build PyTorch: 12.8\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:09:17) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA L20\nNvidia driver version: 550.67\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             180\nOn-line CPU(s) list:                0-179\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8457C\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 45\nSocket(s):                          2\nStepping:                           8\nBogoMIPS:                           5200.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          4.2 MiB (90 instances)\nL1i cache:                          2.8 MiB (90 instances)\nL2 cache:                           180 MiB (90 instances)\nL3 cache:                           195 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-89\nNUMA node1 CPU(s):                  90-179\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Unknown: No mitigations\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] numpy==2.3.2\n[pip3] nvidia-cublas-cu12==12.8.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.8.90\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.93\n[pip3] nvidia-cuda-runtime-cu12==12.8.90\n[pip3] nvidia-cudnn-cu12==9.10.2.21\n[pip3] nvidia-cufft-cu12==11.3.3.83\n[pip3] nvidia-curand-cu12==10.3.9.90\n[pip3] nvidia-cusolver-cu12==11.7.3.90\n[pip3] nvidia-cusparse-cu12==12.5.8.93\n[pip3] nvidia-cusparselt-cu12==0.7.1\n[pip3] nvidia-nccl-cu12==2.27.3\n[pip3] nvidia-nvjitlink-cu12==12.8.93\n[pip3] nvidia-nvtx-cu12==12.8.90\n[pip3] torch==2.8.0\n[pip3] triton==3.4.0\n[conda] blas                      1.0                         mkl    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n[conda] mkl                       2023.1.0         h213fc3f_46344    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n[conda] mkl-service               2.4.0           py312h5eee18b_2    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n[conda] mkl_fft                   1.3.11          py312h5eee18b_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n[conda] mkl_random                1.2.8           py312h526ad5a_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n[conda] numpy                     2.3.2                    pypi_0    pypi\n[conda] numpy-base                2.3.1           py312h06ae042_0    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n[conda] nvidia-cublas-cu12        12.8.4.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.8.90                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.8.93                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.8.90                  pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.10.2.21                pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.3.3.83                pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.9.90                pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.7.3.90                pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.5.8.93                pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.7.1                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.27.3                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.8.93                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.8.90                  pypi_0    pypi\n[conda] torch                     2.8.0                    pypi_0    pypi\n[conda] triton                    3.4.0                    pypi_0    pypi",
    "transformers_version": "4.52.4",
    "upper_git_hash": null,
    "tokenizer_pad_token": [
        "<|endoftext|>",
        "0"
    ],
    "tokenizer_eos_token": [
        "<|endoftext|>",
        "0"
    ],
    "tokenizer_bos_token": [
        "<|endoftext|>",
        "0"
    ],
    "eot_token_id": 0,
    "max_length": 2048,
    "task_hashes": {},
    "model_source": "recursive_lm",
    "model_name": "/root/mixture_of_recursions/results/pretrain/250823_pretrain_smollm-360m_rec2_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001",
    "model_name_sanitized": "__root__mixture_of_recursions__results__pretrain__250823_pretrain_smollm-360m_rec2_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001",
    "system_instruction": null,
    "system_instruction_sha": null,
    "fewshot_as_multiturn": false,
    "chat_template": null,
    "chat_template_sha": null,
    "start_time": 42358254.59185417,
    "end_time": 42359360.18501038,
    "total_evaluation_time_seconds": "1105.5931562110782"
}