{
    "results": {
        "arc_challenge": {
            "acc,none": 0.19539249146757678,
            "acc_stderr,none": 0.011586907189952898,
            "acc_norm,none": 0.2354948805460751,
            "acc_norm_stderr,none": 0.01239945185500478
        },
        "arc_easy": {
            "acc,none": 0.47685185185185186,
            "acc_stderr,none": 0.010248782484554448,
            "acc_norm,none": 0.43308080808080807,
            "acc_norm_stderr,none": 0.010167478013701743
        },
        "hellaswag": {
            "acc,none": 0.280920135431189,
            "acc_stderr,none": 0.004485300194072218,
            "acc_norm,none": 0.2980481975702051,
            "acc_norm_stderr,none": 0.0045646597750764175
        },
        "lambada_openai": {
            "perplexity,none": 244.4421869145443,
            "perplexity_stderr,none": 12.413471859288798,
            "acc,none": 0.20337667378226276,
            "acc_stderr,none": 0.005607756564321473
        },
        "mmlu": {
            "acc,none": 0.22902720410197977,
            "acc_stderr,none": 0.0035405031283773656
        },
        "mmlu_humanities": {
            "acc,none": 0.24250797024442083,
            "acc_stderr,none": 0.006246364786693872
        },
        "mmlu_formal_logic": {
            "acc,none": 0.2857142857142857,
            "acc_stderr,none": 0.04040610178208843
        },
        "mmlu_high_school_european_history": {
            "acc,none": 0.22424242424242424,
            "acc_stderr,none": 0.03256866661681102
        },
        "mmlu_high_school_us_history": {
            "acc,none": 0.2549019607843137,
            "acc_stderr,none": 0.030587591351604302
        },
        "mmlu_high_school_world_history": {
            "acc,none": 0.270042194092827,
            "acc_stderr,none": 0.02890072190629346
        },
        "mmlu_international_law": {
            "acc,none": 0.2396694214876033,
            "acc_stderr,none": 0.03896878985070412
        },
        "mmlu_jurisprudence": {
            "acc,none": 0.26851851851851855,
            "acc_stderr,none": 0.04284467968052193
        },
        "mmlu_logical_fallacies": {
            "acc,none": 0.22085889570552147,
            "acc_stderr,none": 0.032591773927421734
        },
        "mmlu_moral_disputes": {
            "acc,none": 0.24566473988439305,
            "acc_stderr,none": 0.023176298203992085
        },
        "mmlu_moral_scenarios": {
            "acc,none": 0.23798882681564246,
            "acc_stderr,none": 0.014242630070574904
        },
        "mmlu_philosophy": {
            "acc,none": 0.1864951768488746,
            "acc_stderr,none": 0.022122439772480733
        },
        "mmlu_prehistory": {
            "acc,none": 0.21604938271604937,
            "acc_stderr,none": 0.02289916291844576
        },
        "mmlu_professional_law": {
            "acc,none": 0.2457627118644068,
            "acc_stderr,none": 0.010996156635142657
        },
        "mmlu_world_religions": {
            "acc,none": 0.3216374269005848,
            "acc_stderr,none": 0.03582529442573121
        },
        "mmlu_other": {
            "acc,none": 0.23817186997103315,
            "acc_stderr,none": 0.0076254323401286055
        },
        "mmlu_business_ethics": {
            "acc,none": 0.3,
            "acc_stderr,none": 0.04605661864718382
        },
        "mmlu_clinical_knowledge": {
            "acc,none": 0.21509433962264152,
            "acc_stderr,none": 0.02528839450289141
        },
        "mmlu_college_medicine": {
            "acc,none": 0.20809248554913296,
            "acc_stderr,none": 0.030952890217749857
        },
        "mmlu_global_facts": {
            "acc,none": 0.18,
            "acc_stderr,none": 0.03861229196653691
        },
        "mmlu_human_aging": {
            "acc,none": 0.31390134529147984,
            "acc_stderr,none": 0.031146796482972486
        },
        "mmlu_management": {
            "acc,none": 0.17475728155339806,
            "acc_stderr,none": 0.03760178006026618
        },
        "mmlu_marketing": {
            "acc,none": 0.2863247863247863,
            "acc_stderr,none": 0.029614323690456627
        },
        "mmlu_medical_genetics": {
            "acc,none": 0.3,
            "acc_stderr,none": 0.04605661864718382
        },
        "mmlu_miscellaneous": {
            "acc,none": 0.23371647509578544,
            "acc_stderr,none": 0.015133383278988898
        },
        "mmlu_nutrition": {
            "acc,none": 0.2222222222222222,
            "acc_stderr,none": 0.023805186524888222
        },
        "mmlu_professional_accounting": {
            "acc,none": 0.23049645390070922,
            "acc_stderr,none": 0.025123739226872346
        },
        "mmlu_professional_medicine": {
            "acc,none": 0.1875,
            "acc_stderr,none": 0.023709788253811766
        },
        "mmlu_virology": {
            "acc,none": 0.28313253012048195,
            "acc_stderr,none": 0.035072954313705176
        },
        "mmlu_social_sciences": {
            "acc,none": 0.216769580760481,
            "acc_stderr,none": 0.007425644186657176
        },
        "mmlu_econometrics": {
            "acc,none": 0.23684210526315788,
            "acc_stderr,none": 0.03999423879281335
        },
        "mmlu_high_school_geography": {
            "acc,none": 0.17676767676767677,
            "acc_stderr,none": 0.027178752639044908
        },
        "mmlu_high_school_government_and_politics": {
            "acc,none": 0.19689119170984457,
            "acc_stderr,none": 0.028697873971860723
        },
        "mmlu_high_school_macroeconomics": {
            "acc,none": 0.20256410256410257,
            "acc_stderr,none": 0.020377660970371435
        },
        "mmlu_high_school_microeconomics": {
            "acc,none": 0.21008403361344538,
            "acc_stderr,none": 0.026461398717471864
        },
        "mmlu_high_school_psychology": {
            "acc,none": 0.1926605504587156,
            "acc_stderr,none": 0.016909276884936104
        },
        "mmlu_human_sexuality": {
            "acc,none": 0.25190839694656486,
            "acc_stderr,none": 0.03807387116306089
        },
        "mmlu_professional_psychology": {
            "acc,none": 0.25163398692810457,
            "acc_stderr,none": 0.017555818091322294
        },
        "mmlu_public_relations": {
            "acc,none": 0.21818181818181817,
            "acc_stderr,none": 0.03955932861795833
        },
        "mmlu_security_studies": {
            "acc,none": 0.18775510204081633,
            "acc_stderr,none": 0.025000256039546167
        },
        "mmlu_sociology": {
            "acc,none": 0.24378109452736318,
            "acc_stderr,none": 0.03036049015401464
        },
        "mmlu_us_foreign_policy": {
            "acc,none": 0.27,
            "acc_stderr,none": 0.04461960433384737
        },
        "mmlu_stem": {
            "acc,none": 0.2118617189977799,
            "acc_stderr,none": 0.007263181102290584
        },
        "mmlu_abstract_algebra": {
            "acc,none": 0.22,
            "acc_stderr,none": 0.041633319989322654
        },
        "mmlu_anatomy": {
            "acc,none": 0.1925925925925926,
            "acc_stderr,none": 0.03406542058502651
        },
        "mmlu_astronomy": {
            "acc,none": 0.17763157894736842,
            "acc_stderr,none": 0.031103182383123377
        },
        "mmlu_college_biology": {
            "acc,none": 0.2569444444444444,
            "acc_stderr,none": 0.03653946969442102
        },
        "mmlu_college_chemistry": {
            "acc,none": 0.18,
            "acc_stderr,none": 0.03861229196653691
        },
        "mmlu_college_computer_science": {
            "acc,none": 0.25,
            "acc_stderr,none": 0.04351941398892446
        },
        "mmlu_college_mathematics": {
            "acc,none": 0.21,
            "acc_stderr,none": 0.040936018074033236
        },
        "mmlu_college_physics": {
            "acc,none": 0.21568627450980393,
            "acc_stderr,none": 0.04092563958237658
        },
        "mmlu_computer_security": {
            "acc,none": 0.29,
            "acc_stderr,none": 0.045604802157206865
        },
        "mmlu_conceptual_physics": {
            "acc,none": 0.25957446808510637,
            "acc_stderr,none": 0.02865917937429237
        },
        "mmlu_electrical_engineering": {
            "acc,none": 0.2413793103448276,
            "acc_stderr,none": 0.035659981741353035
        },
        "mmlu_elementary_mathematics": {
            "acc,none": 0.20899470899470898,
            "acc_stderr,none": 0.020940481565334935
        },
        "mmlu_high_school_biology": {
            "acc,none": 0.1774193548387097,
            "acc_stderr,none": 0.021732540689329255
        },
        "mmlu_high_school_chemistry": {
            "acc,none": 0.15270935960591134,
            "acc_stderr,none": 0.025308904539380683
        },
        "mmlu_high_school_computer_science": {
            "acc,none": 0.25,
            "acc_stderr,none": 0.04351941398892446
        },
        "mmlu_high_school_mathematics": {
            "acc,none": 0.2111111111111111,
            "acc_stderr,none": 0.02488211685765511
        },
        "mmlu_high_school_physics": {
            "acc,none": 0.1986754966887417,
            "acc_stderr,none": 0.032578473844367795
        },
        "mmlu_high_school_statistics": {
            "acc,none": 0.1527777777777778,
            "acc_stderr,none": 0.024536326026134234
        },
        "mmlu_machine_learning": {
            "acc,none": 0.3125,
            "acc_stderr,none": 0.043994650575715215
        },
        "mmlu_continuation": {
            "acc,none": 0.2494658880501353,
            "acc_stderr,none": 0.0036371689791193738
        },
        "humanities": {
            "acc,none": 0.2361317747077577,
            "acc_stderr,none": 0.006192383957930416
        },
        "mmlu_formal_logic_continuation": {
            "acc,none": 0.24603174603174602,
            "acc_stderr,none": 0.03852273364924316,
            "acc_norm,none": 0.23809523809523808,
            "acc_norm_stderr,none": 0.03809523809523809
        },
        "mmlu_high_school_european_history_continuation": {
            "acc,none": 0.23636363636363636,
            "acc_stderr,none": 0.03317505930009174,
            "acc_norm,none": 0.3878787878787879,
            "acc_norm_stderr,none": 0.03804913653971008
        },
        "mmlu_high_school_us_history_continuation": {
            "acc,none": 0.23529411764705882,
            "acc_stderr,none": 0.029771775228145628,
            "acc_norm,none": 0.3235294117647059,
            "acc_norm_stderr,none": 0.032834720561085606
        },
        "mmlu_high_school_world_history_continuation": {
            "acc,none": 0.22784810126582278,
            "acc_stderr,none": 0.027303484599069387,
            "acc_norm,none": 0.2869198312236287,
            "acc_norm_stderr,none": 0.02944377302259465
        },
        "mmlu_international_law_continuation": {
            "acc,none": 0.15702479338842976,
            "acc_stderr,none": 0.03321244842547127,
            "acc_norm,none": 0.256198347107438,
            "acc_norm_stderr,none": 0.03984979653302874
        },
        "mmlu_jurisprudence_continuation": {
            "acc,none": 0.2037037037037037,
            "acc_stderr,none": 0.038935425188248496,
            "acc_norm,none": 0.23148148148148148,
            "acc_norm_stderr,none": 0.040774947092526284
        },
        "mmlu_logical_fallacies_continuation": {
            "acc,none": 0.27607361963190186,
            "acc_stderr,none": 0.035123852837050454,
            "acc_norm,none": 0.3128834355828221,
            "acc_norm_stderr,none": 0.03642914578292403
        },
        "mmlu_moral_disputes_continuation": {
            "acc,none": 0.23410404624277456,
            "acc_stderr,none": 0.022797110278071152,
            "acc_norm,none": 0.21098265895953758,
            "acc_norm_stderr,none": 0.02196630994704318
        },
        "mmlu_moral_scenarios_continuation": {
            "acc,none": 0.23910614525139665,
            "acc_stderr,none": 0.014265554192331054,
            "acc_norm,none": 0.27262569832402234,
            "acc_norm_stderr,none": 0.014893391735249532
        },
        "mmlu_philosophy_continuation": {
            "acc,none": 0.2315112540192926,
            "acc_stderr,none": 0.023956532766639154,
            "acc_norm,none": 0.2540192926045016,
            "acc_norm_stderr,none": 0.024723861504771655
        },
        "mmlu_prehistory_continuation": {
            "acc,none": 0.28703703703703703,
            "acc_stderr,none": 0.025171041915309698,
            "acc_norm,none": 0.18518518518518517,
            "acc_norm_stderr,none": 0.021613809395224805
        },
        "mmlu_professional_law_continuation": {
            "acc,none": 0.23272490221642764,
            "acc_stderr,none": 0.010792595553888416,
            "acc_norm,none": 0.2516297262059974,
            "acc_norm_stderr,none": 0.011083276280441872
        },
        "mmlu_world_religions_continuation": {
            "acc,none": 0.21052631578947367,
            "acc_stderr,none": 0.03126781714663182,
            "acc_norm,none": 0.30409356725146197,
            "acc_norm_stderr,none": 0.03528211258245229
        },
        "other": {
            "acc,none": 0.2745413582233666,
            "acc_stderr,none": 0.007968365534776428
        },
        "mmlu_business_ethics_continuation": {
            "acc,none": 0.38,
            "acc_stderr,none": 0.04878317312145634,
            "acc_norm,none": 0.39,
            "acc_norm_stderr,none": 0.04902071300001973
        },
        "mmlu_clinical_knowledge_continuation": {
            "acc,none": 0.2,
            "acc_stderr,none": 0.024618298195866552,
            "acc_norm,none": 0.2641509433962264,
            "acc_norm_stderr,none": 0.02713429162874164
        },
        "mmlu_college_medicine_continuation": {
            "acc,none": 0.2254335260115607,
            "acc_stderr,none": 0.03186209851641147,
            "acc_norm,none": 0.2254335260115607,
            "acc_norm_stderr,none": 0.03186209851641147
        },
        "mmlu_global_facts_continuation": {
            "acc,none": 0.28,
            "acc_stderr,none": 0.045126085985421296,
            "acc_norm,none": 0.28,
            "acc_norm_stderr,none": 0.045126085985421296
        },
        "mmlu_human_aging_continuation": {
            "acc,none": 0.34080717488789236,
            "acc_stderr,none": 0.03181149747055356,
            "acc_norm,none": 0.2600896860986547,
            "acc_norm_stderr,none": 0.029442495585857424
        },
        "mmlu_management_continuation": {
            "acc,none": 0.22330097087378642,
            "acc_stderr,none": 0.041235531898914324,
            "acc_norm,none": 0.30097087378640774,
            "acc_norm_stderr,none": 0.045416094465039504
        },
        "mmlu_marketing_continuation": {
            "acc,none": 0.3247863247863248,
            "acc_stderr,none": 0.030679022765498814,
            "acc_norm,none": 0.32905982905982906,
            "acc_norm_stderr,none": 0.030782321577688235
        },
        "mmlu_medical_genetics_continuation": {
            "acc,none": 0.24,
            "acc_stderr,none": 0.04292346959909278,
            "acc_norm,none": 0.3,
            "acc_norm_stderr,none": 0.04605661864718382
        },
        "mmlu_miscellaneous_continuation": {
            "acc,none": 0.3218390804597701,
            "acc_stderr,none": 0.016706381415058022,
            "acc_norm,none": 0.30395913154533843,
            "acc_norm_stderr,none": 0.016448321686768984
        },
        "mmlu_nutrition_continuation": {
            "acc,none": 0.21241830065359477,
            "acc_stderr,none": 0.023420375478296097,
            "acc_norm,none": 0.28431372549019607,
            "acc_norm_stderr,none": 0.025829163272757385
        },
        "mmlu_professional_accounting_continuation": {
            "acc,none": 0.25886524822695034,
            "acc_stderr,none": 0.026129572527180806,
            "acc_norm,none": 0.26595744680851063,
            "acc_norm_stderr,none": 0.026358065698880644
        },
        "mmlu_professional_medicine_continuation": {
            "acc,none": 0.23897058823529413,
            "acc_stderr,none": 0.025905280644893044,
            "acc_norm,none": 0.27941176470588236,
            "acc_norm_stderr,none": 0.02725720260611497
        },
        "mmlu_virology_continuation": {
            "acc,none": 0.2469879518072289,
            "acc_stderr,none": 0.033573519820645346,
            "acc_norm,none": 0.29518072289156627,
            "acc_norm_stderr,none": 0.035509201856896336
        },
        "social sciences": {
            "acc,none": 0.2694182645433864,
            "acc_stderr,none": 0.007971100685307
        },
        "mmlu_econometrics_continuation": {
            "acc,none": 0.16666666666666666,
            "acc_stderr,none": 0.03505859682597261,
            "acc_norm,none": 0.20175438596491227,
            "acc_norm_stderr,none": 0.03775205013583637
        },
        "mmlu_high_school_geography_continuation": {
            "acc,none": 0.25757575757575757,
            "acc_stderr,none": 0.031156269519646826,
            "acc_norm,none": 0.3282828282828283,
            "acc_norm_stderr,none": 0.03345678422756777
        },
        "mmlu_high_school_government_and_politics_continuation": {
            "acc,none": 0.23834196891191708,
            "acc_stderr,none": 0.030748905363909895,
            "acc_norm,none": 0.29533678756476683,
            "acc_norm_stderr,none": 0.032922966391551386
        },
        "mmlu_high_school_macroeconomics_continuation": {
            "acc,none": 0.2076923076923077,
            "acc_stderr,none": 0.020567539567246756,
            "acc_norm,none": 0.25384615384615383,
            "acc_norm_stderr,none": 0.022066054378726312
        },
        "mmlu_high_school_microeconomics_continuation": {
            "acc,none": 0.2689075630252101,
            "acc_stderr,none": 0.028801392193631228,
            "acc_norm,none": 0.3487394957983193,
            "acc_norm_stderr,none": 0.03095663632856658
        },
        "mmlu_high_school_psychology_continuation": {
            "acc,none": 0.3137614678899083,
            "acc_stderr,none": 0.01989472334146921,
            "acc_norm,none": 0.28807339449541286,
            "acc_norm_stderr,none": 0.019416445892635966
        },
        "mmlu_human_sexuality_continuation": {
            "acc,none": 0.32061068702290074,
            "acc_stderr,none": 0.04093329229834282,
            "acc_norm,none": 0.31297709923664124,
            "acc_norm_stderr,none": 0.04066962905677697
        },
        "mmlu_professional_psychology_continuation": {
            "acc,none": 0.2696078431372549,
            "acc_stderr,none": 0.017952449196987838,
            "acc_norm,none": 0.28104575163398693,
            "acc_norm_stderr,none": 0.018185218954318044
        },
        "mmlu_public_relations_continuation": {
            "acc,none": 0.37272727272727274,
            "acc_stderr,none": 0.0463138131942546,
            "acc_norm,none": 0.23636363636363636,
            "acc_norm_stderr,none": 0.04069306319721375
        },
        "mmlu_security_studies_continuation": {
            "acc,none": 0.3183673469387755,
            "acc_stderr,none": 0.029822533793982028,
            "acc_norm,none": 0.22040816326530613,
            "acc_norm_stderr,none": 0.026537045312145312
        },
        "mmlu_sociology_continuation": {
            "acc,none": 0.23383084577114427,
            "acc_stderr,none": 0.02992941540834832,
            "acc_norm,none": 0.24378109452736318,
            "acc_norm_stderr,none": 0.03036049015401464
        },
        "mmlu_us_foreign_policy_continuation": {
            "acc,none": 0.24,
            "acc_stderr,none": 0.04292346959909278,
            "acc_norm,none": 0.24,
            "acc_norm_stderr,none": 0.04292346959909278
        },
        "stem": {
            "acc,none": 0.22518236600063432,
            "acc_stderr,none": 0.007404383298081147
        },
        "mmlu_abstract_algebra_continuation": {
            "acc,none": 0.2,
            "acc_stderr,none": 0.04020151261036849,
            "acc_norm,none": 0.27,
            "acc_norm_stderr,none": 0.04461960433384737
        },
        "mmlu_anatomy_continuation": {
            "acc,none": 0.28888888888888886,
            "acc_stderr,none": 0.0391545063041425,
            "acc_norm,none": 0.2814814814814815,
            "acc_norm_stderr,none": 0.03885004245800249
        },
        "mmlu_astronomy_continuation": {
            "acc,none": 0.2236842105263158,
            "acc_stderr,none": 0.03391160934343604,
            "acc_norm,none": 0.27631578947368424,
            "acc_norm_stderr,none": 0.03639057569952929
        },
        "mmlu_college_biology_continuation": {
            "acc,none": 0.3125,
            "acc_stderr,none": 0.038760854559127644,
            "acc_norm,none": 0.2777777777777778,
            "acc_norm_stderr,none": 0.037455547914624555
        },
        "mmlu_college_chemistry_continuation": {
            "acc,none": 0.24,
            "acc_stderr,none": 0.04292346959909278,
            "acc_norm,none": 0.23,
            "acc_norm_stderr,none": 0.04229525846816507
        },
        "mmlu_college_computer_science_continuation": {
            "acc,none": 0.21,
            "acc_stderr,none": 0.040936018074033236,
            "acc_norm,none": 0.23,
            "acc_norm_stderr,none": 0.04229525846816507
        },
        "mmlu_college_mathematics_continuation": {
            "acc,none": 0.12,
            "acc_stderr,none": 0.032659863237109045,
            "acc_norm,none": 0.23,
            "acc_norm_stderr,none": 0.04229525846816507
        },
        "mmlu_college_physics_continuation": {
            "acc,none": 0.12745098039215685,
            "acc_stderr,none": 0.03318224921942079,
            "acc_norm,none": 0.18627450980392157,
            "acc_norm_stderr,none": 0.03873958714149351
        },
        "mmlu_computer_security_continuation": {
            "acc,none": 0.27,
            "acc_stderr,none": 0.04461960433384737,
            "acc_norm,none": 0.31,
            "acc_norm_stderr,none": 0.04648231987117317
        },
        "mmlu_conceptual_physics_continuation": {
            "acc,none": 0.2723404255319149,
            "acc_stderr,none": 0.029101290698386736,
            "acc_norm,none": 0.2170212765957447,
            "acc_norm_stderr,none": 0.026947483121496238
        },
        "mmlu_electrical_engineering_continuation": {
            "acc,none": 0.21379310344827587,
            "acc_stderr,none": 0.034165204477475515,
            "acc_norm,none": 0.2413793103448276,
            "acc_norm_stderr,none": 0.035659981741353035
        },
        "mmlu_elementary_mathematics_continuation": {
            "acc,none": 0.2328042328042328,
            "acc_stderr,none": 0.021765961672154555,
            "acc_norm,none": 0.23544973544973544,
            "acc_norm_stderr,none": 0.021851509822031694
        },
        "mmlu_high_school_biology_continuation": {
            "acc,none": 0.25161290322580643,
            "acc_stderr,none": 0.02468597928624002,
            "acc_norm,none": 0.29354838709677417,
            "acc_norm_stderr,none": 0.02590608702131934
        },
        "mmlu_high_school_chemistry_continuation": {
            "acc,none": 0.1724137931034483,
            "acc_stderr,none": 0.026577672183036617,
            "acc_norm,none": 0.20689655172413793,
            "acc_norm_stderr,none": 0.028501378167893957
        },
        "mmlu_high_school_computer_science_continuation": {
            "acc,none": 0.21,
            "acc_stderr,none": 0.040936018074033236,
            "acc_norm,none": 0.27,
            "acc_norm_stderr,none": 0.04461960433384737
        },
        "mmlu_high_school_mathematics_continuation": {
            "acc,none": 0.12962962962962962,
            "acc_stderr,none": 0.020479910253320677,
            "acc_norm,none": 0.17777777777777778,
            "acc_norm_stderr,none": 0.02331080126064486
        },
        "mmlu_high_school_physics_continuation": {
            "acc,none": 0.26490066225165565,
            "acc_stderr,none": 0.036030385453603826,
            "acc_norm,none": 0.2582781456953642,
            "acc_norm_stderr,none": 0.03573705314763455
        },
        "mmlu_high_school_statistics_continuation": {
            "acc,none": 0.25,
            "acc_stderr,none": 0.029531221160930918,
            "acc_norm,none": 0.2777777777777778,
            "acc_norm_stderr,none": 0.030546745264953202
        },
        "mmlu_machine_learning_continuation": {
            "acc,none": 0.25892857142857145,
            "acc_stderr,none": 0.04157751539865629,
            "acc_norm,none": 0.19642857142857142,
            "acc_norm_stderr,none": 0.03770970049347019
        },
        "openbookqa": {
            "acc,none": 0.178,
            "acc_stderr,none": 0.01712362218906232,
            "acc_norm,none": 0.288,
            "acc_norm_stderr,none": 0.020271503835075255
        },
        "piqa": {
            "acc,none": 0.6055495103373232,
            "acc_stderr,none": 0.011402931101558359,
            "acc_norm,none": 0.6039173014145811,
            "acc_norm_stderr,none": 0.011411089031912588
        },
        "winogrande": {
            "acc,none": 0.4988161010260458,
            "acc_stderr,none": 0.014052446290528871
        }
    },
    "groups": {
        "mmlu": {
            "acc,none": 0.22902720410197977,
            "acc_stderr,none": 0.0035405031283773656
        },
        "mmlu_humanities": {
            "acc,none": 0.24250797024442083,
            "acc_stderr,none": 0.006246364786693872
        },
        "mmlu_other": {
            "acc,none": 0.23817186997103315,
            "acc_stderr,none": 0.0076254323401286055
        },
        "mmlu_social_sciences": {
            "acc,none": 0.216769580760481,
            "acc_stderr,none": 0.007425644186657176
        },
        "mmlu_stem": {
            "acc,none": 0.2118617189977799,
            "acc_stderr,none": 0.007263181102290584
        },
        "mmlu_continuation": {
            "acc,none": 0.2494658880501353,
            "acc_stderr,none": 0.0036371689791193738
        },
        "humanities": {
            "acc,none": 0.2361317747077577,
            "acc_stderr,none": 0.006192383957930416
        },
        "other": {
            "acc,none": 0.2745413582233666,
            "acc_stderr,none": 0.007968365534776428
        },
        "social sciences": {
            "acc,none": 0.2694182645433864,
            "acc_stderr,none": 0.007971100685307
        },
        "stem": {
            "acc,none": 0.22518236600063432,
            "acc_stderr,none": 0.007404383298081147
        }
    },
    "group_subtasks": {
        "arc_challenge": [],
        "arc_easy": [],
        "hellaswag": [],
        "lambada_openai": [],
        "mmlu_humanities": [
            "mmlu_formal_logic",
            "mmlu_high_school_european_history",
            "mmlu_high_school_us_history",
            "mmlu_high_school_world_history",
            "mmlu_international_law",
            "mmlu_jurisprudence",
            "mmlu_logical_fallacies",
            "mmlu_moral_disputes",
            "mmlu_moral_scenarios",
            "mmlu_philosophy",
            "mmlu_prehistory",
            "mmlu_professional_law",
            "mmlu_world_religions"
        ],
        "mmlu_social_sciences": [
            "mmlu_econometrics",
            "mmlu_high_school_geography",
            "mmlu_high_school_government_and_politics",
            "mmlu_high_school_macroeconomics",
            "mmlu_high_school_microeconomics",
            "mmlu_high_school_psychology",
            "mmlu_human_sexuality",
            "mmlu_professional_psychology",
            "mmlu_public_relations",
            "mmlu_security_studies",
            "mmlu_sociology",
            "mmlu_us_foreign_policy"
        ],
        "mmlu_other": [
            "mmlu_business_ethics",
            "mmlu_clinical_knowledge",
            "mmlu_college_medicine",
            "mmlu_global_facts",
            "mmlu_human_aging",
            "mmlu_management",
            "mmlu_marketing",
            "mmlu_medical_genetics",
            "mmlu_miscellaneous",
            "mmlu_nutrition",
            "mmlu_professional_accounting",
            "mmlu_professional_medicine",
            "mmlu_virology"
        ],
        "mmlu_stem": [
            "mmlu_abstract_algebra",
            "mmlu_anatomy",
            "mmlu_astronomy",
            "mmlu_college_biology",
            "mmlu_college_chemistry",
            "mmlu_college_computer_science",
            "mmlu_college_mathematics",
            "mmlu_college_physics",
            "mmlu_computer_security",
            "mmlu_conceptual_physics",
            "mmlu_electrical_engineering",
            "mmlu_elementary_mathematics",
            "mmlu_high_school_biology",
            "mmlu_high_school_chemistry",
            "mmlu_high_school_computer_science",
            "mmlu_high_school_mathematics",
            "mmlu_high_school_physics",
            "mmlu_high_school_statistics",
            "mmlu_machine_learning"
        ],
        "mmlu": [
            "mmlu_stem",
            "mmlu_other",
            "mmlu_social_sciences",
            "mmlu_humanities"
        ],
        "humanities": [
            "mmlu_formal_logic_continuation",
            "mmlu_high_school_european_history_continuation",
            "mmlu_high_school_us_history_continuation",
            "mmlu_high_school_world_history_continuation",
            "mmlu_international_law_continuation",
            "mmlu_jurisprudence_continuation",
            "mmlu_logical_fallacies_continuation",
            "mmlu_moral_disputes_continuation",
            "mmlu_moral_scenarios_continuation",
            "mmlu_philosophy_continuation",
            "mmlu_prehistory_continuation",
            "mmlu_professional_law_continuation",
            "mmlu_world_religions_continuation"
        ],
        "social sciences": [
            "mmlu_econometrics_continuation",
            "mmlu_high_school_geography_continuation",
            "mmlu_high_school_government_and_politics_continuation",
            "mmlu_high_school_macroeconomics_continuation",
            "mmlu_high_school_microeconomics_continuation",
            "mmlu_high_school_psychology_continuation",
            "mmlu_human_sexuality_continuation",
            "mmlu_professional_psychology_continuation",
            "mmlu_public_relations_continuation",
            "mmlu_security_studies_continuation",
            "mmlu_sociology_continuation",
            "mmlu_us_foreign_policy_continuation"
        ],
        "other": [
            "mmlu_business_ethics_continuation",
            "mmlu_clinical_knowledge_continuation",
            "mmlu_college_medicine_continuation",
            "mmlu_global_facts_continuation",
            "mmlu_human_aging_continuation",
            "mmlu_management_continuation",
            "mmlu_marketing_continuation",
            "mmlu_medical_genetics_continuation",
            "mmlu_miscellaneous_continuation",
            "mmlu_nutrition_continuation",
            "mmlu_professional_accounting_continuation",
            "mmlu_professional_medicine_continuation",
            "mmlu_virology_continuation"
        ],
        "stem": [
            "mmlu_abstract_algebra_continuation",
            "mmlu_anatomy_continuation",
            "mmlu_astronomy_continuation",
            "mmlu_college_biology_continuation",
            "mmlu_college_chemistry_continuation",
            "mmlu_college_computer_science_continuation",
            "mmlu_college_mathematics_continuation",
            "mmlu_college_physics_continuation",
            "mmlu_computer_security_continuation",
            "mmlu_conceptual_physics_continuation",
            "mmlu_electrical_engineering_continuation",
            "mmlu_elementary_mathematics_continuation",
            "mmlu_high_school_biology_continuation",
            "mmlu_high_school_chemistry_continuation",
            "mmlu_high_school_computer_science_continuation",
            "mmlu_high_school_mathematics_continuation",
            "mmlu_high_school_physics_continuation",
            "mmlu_high_school_statistics_continuation",
            "mmlu_machine_learning_continuation"
        ],
        "mmlu_continuation": [
            "stem",
            "other",
            "social sciences",
            "humanities"
        ],
        "openbookqa": [],
        "piqa": [],
        "winogrande": []
    },
    "configs": {
        "arc_challenge": {
            "task": "arc_challenge",
            "tag": [
                "ai2_arc"
            ],
            "dataset_path": "allenai/ai2_arc",
            "dataset_name": "ARC-Challenge",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{question}}\nAnswer:",
            "doc_to_target": "{{choices.label.index(answerKey)}}",
            "doc_to_choice": "{{choices.text}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "Question: {{question}}\nAnswer:",
            "metadata": {
                "version": 1.0
            }
        },
        "arc_easy": {
            "task": "arc_easy",
            "tag": [
                "ai2_arc"
            ],
            "dataset_path": "allenai/ai2_arc",
            "dataset_name": "ARC-Easy",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "Question: {{question}}\nAnswer:",
            "doc_to_target": "{{choices.label.index(answerKey)}}",
            "doc_to_choice": "{{choices.text}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "Question: {{question}}\nAnswer:",
            "metadata": {
                "version": 1.0
            }
        },
        "hellaswag": {
            "task": "hellaswag",
            "tag": [
                "multiple_choice"
            ],
            "dataset_path": "Rowan/hellaswag",
            "training_split": "train",
            "validation_split": "validation",
            "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _process_doc(doc):\n        ctx = doc[\"ctx_a\"] + \" \" + doc[\"ctx_b\"].capitalize()\n        out_doc = {\n            \"query\": preprocess(doc[\"activity_label\"] + \": \" + ctx),\n            \"choices\": [preprocess(ending) for ending in doc[\"endings\"]],\n            \"gold\": int(doc[\"label\"]),\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n",
            "doc_to_text": "{{query}}",
            "doc_to_target": "{{label}}",
            "doc_to_choice": "choices",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "lambada_openai": {
            "task": "lambada_openai",
            "tag": [
                "lambada"
            ],
            "dataset_path": "EleutherAI/lambada_openai",
            "dataset_name": "default",
            "test_split": "test",
            "doc_to_text": "{{text.split(' ')[:-1]|join(' ')}}",
            "doc_to_target": "{{' '+text.split(' ')[-1]}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "perplexity",
                    "aggregation": "perplexity",
                    "higher_is_better": false
                },
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "loglikelihood",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "{{text}}",
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_abstract_algebra": {
            "task": "mmlu_abstract_algebra",
            "task_alias": "abstract_algebra",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "abstract_algebra",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_abstract_algebra_continuation": {
            "task": "mmlu_abstract_algebra_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "abstract_algebra",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about abstract algebra.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_anatomy": {
            "task": "mmlu_anatomy",
            "task_alias": "anatomy",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "anatomy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_anatomy_continuation": {
            "task": "mmlu_anatomy_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "anatomy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about anatomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_astronomy": {
            "task": "mmlu_astronomy",
            "task_alias": "astronomy",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "astronomy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_astronomy_continuation": {
            "task": "mmlu_astronomy_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "astronomy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about astronomy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_business_ethics": {
            "task": "mmlu_business_ethics",
            "task_alias": "business_ethics",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "business_ethics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_business_ethics_continuation": {
            "task": "mmlu_business_ethics_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "business_ethics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about business ethics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_clinical_knowledge": {
            "task": "mmlu_clinical_knowledge",
            "task_alias": "clinical_knowledge",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "clinical_knowledge",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_clinical_knowledge_continuation": {
            "task": "mmlu_clinical_knowledge_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "clinical_knowledge",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about clinical knowledge.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_biology": {
            "task": "mmlu_college_biology",
            "task_alias": "college_biology",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_biology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_biology_continuation": {
            "task": "mmlu_college_biology_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_biology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_chemistry": {
            "task": "mmlu_college_chemistry",
            "task_alias": "college_chemistry",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_chemistry",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_chemistry_continuation": {
            "task": "mmlu_college_chemistry_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_chemistry",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college chemistry.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_computer_science": {
            "task": "mmlu_college_computer_science",
            "task_alias": "college_computer_science",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_computer_science",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_computer_science_continuation": {
            "task": "mmlu_college_computer_science_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_computer_science",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college computer science.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_mathematics": {
            "task": "mmlu_college_mathematics",
            "task_alias": "college_mathematics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_mathematics_continuation": {
            "task": "mmlu_college_mathematics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_medicine": {
            "task": "mmlu_college_medicine",
            "task_alias": "college_medicine",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_medicine",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_medicine_continuation": {
            "task": "mmlu_college_medicine_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_medicine",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_physics": {
            "task": "mmlu_college_physics",
            "task_alias": "college_physics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_college_physics_continuation": {
            "task": "mmlu_college_physics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "college_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about college physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_computer_security": {
            "task": "mmlu_computer_security",
            "task_alias": "computer_security",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "computer_security",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_computer_security_continuation": {
            "task": "mmlu_computer_security_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "computer_security",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about computer security.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_conceptual_physics": {
            "task": "mmlu_conceptual_physics",
            "task_alias": "conceptual_physics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "conceptual_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_conceptual_physics_continuation": {
            "task": "mmlu_conceptual_physics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "conceptual_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about conceptual physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_econometrics": {
            "task": "mmlu_econometrics",
            "task_alias": "econometrics",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "econometrics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_econometrics_continuation": {
            "task": "mmlu_econometrics_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "econometrics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about econometrics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_electrical_engineering": {
            "task": "mmlu_electrical_engineering",
            "task_alias": "electrical_engineering",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "electrical_engineering",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_electrical_engineering_continuation": {
            "task": "mmlu_electrical_engineering_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "electrical_engineering",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about electrical engineering.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_elementary_mathematics": {
            "task": "mmlu_elementary_mathematics",
            "task_alias": "elementary_mathematics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "elementary_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_elementary_mathematics_continuation": {
            "task": "mmlu_elementary_mathematics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "elementary_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about elementary mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_formal_logic": {
            "task": "mmlu_formal_logic",
            "task_alias": "formal_logic",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "formal_logic",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_formal_logic_continuation": {
            "task": "mmlu_formal_logic_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "formal_logic",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about formal logic.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_global_facts": {
            "task": "mmlu_global_facts",
            "task_alias": "global_facts",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "global_facts",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_global_facts_continuation": {
            "task": "mmlu_global_facts_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "global_facts",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about global facts.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_biology": {
            "task": "mmlu_high_school_biology",
            "task_alias": "high_school_biology",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_biology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_biology_continuation": {
            "task": "mmlu_high_school_biology_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_biology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school biology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_chemistry": {
            "task": "mmlu_high_school_chemistry",
            "task_alias": "high_school_chemistry",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_chemistry",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_chemistry_continuation": {
            "task": "mmlu_high_school_chemistry_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_chemistry",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school chemistry.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_computer_science": {
            "task": "mmlu_high_school_computer_science",
            "task_alias": "high_school_computer_science",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_computer_science",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_computer_science_continuation": {
            "task": "mmlu_high_school_computer_science_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_computer_science",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school computer science.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_european_history": {
            "task": "mmlu_high_school_european_history",
            "task_alias": "high_school_european_history",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_european_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_european_history_continuation": {
            "task": "mmlu_high_school_european_history_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_european_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school european history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_geography": {
            "task": "mmlu_high_school_geography",
            "task_alias": "high_school_geography",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_geography",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_geography_continuation": {
            "task": "mmlu_high_school_geography_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_geography",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school geography.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_government_and_politics": {
            "task": "mmlu_high_school_government_and_politics",
            "task_alias": "high_school_government_and_politics",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_government_and_politics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_government_and_politics_continuation": {
            "task": "mmlu_high_school_government_and_politics_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_government_and_politics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school government and politics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_macroeconomics": {
            "task": "mmlu_high_school_macroeconomics",
            "task_alias": "high_school_macroeconomics",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_macroeconomics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_macroeconomics_continuation": {
            "task": "mmlu_high_school_macroeconomics_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_macroeconomics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school macroeconomics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_mathematics": {
            "task": "mmlu_high_school_mathematics",
            "task_alias": "high_school_mathematics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_mathematics_continuation": {
            "task": "mmlu_high_school_mathematics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_mathematics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school mathematics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_microeconomics": {
            "task": "mmlu_high_school_microeconomics",
            "task_alias": "high_school_microeconomics",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_microeconomics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_microeconomics_continuation": {
            "task": "mmlu_high_school_microeconomics_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_microeconomics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school microeconomics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_physics": {
            "task": "mmlu_high_school_physics",
            "task_alias": "high_school_physics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_physics_continuation": {
            "task": "mmlu_high_school_physics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_physics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school physics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_psychology": {
            "task": "mmlu_high_school_psychology",
            "task_alias": "high_school_psychology",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_psychology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_psychology_continuation": {
            "task": "mmlu_high_school_psychology_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_psychology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school psychology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_statistics": {
            "task": "mmlu_high_school_statistics",
            "task_alias": "high_school_statistics",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_statistics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_statistics_continuation": {
            "task": "mmlu_high_school_statistics_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_statistics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school statistics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_us_history": {
            "task": "mmlu_high_school_us_history",
            "task_alias": "high_school_us_history",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_us_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_us_history_continuation": {
            "task": "mmlu_high_school_us_history_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_us_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school us history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_world_history": {
            "task": "mmlu_high_school_world_history",
            "task_alias": "high_school_world_history",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_world_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_high_school_world_history_continuation": {
            "task": "mmlu_high_school_world_history_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "high_school_world_history",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about high school world history.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_human_aging": {
            "task": "mmlu_human_aging",
            "task_alias": "human_aging",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "human_aging",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_human_aging_continuation": {
            "task": "mmlu_human_aging_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "human_aging",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about human aging.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_human_sexuality": {
            "task": "mmlu_human_sexuality",
            "task_alias": "human_sexuality",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "human_sexuality",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_human_sexuality_continuation": {
            "task": "mmlu_human_sexuality_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "human_sexuality",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about human sexuality.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_international_law": {
            "task": "mmlu_international_law",
            "task_alias": "international_law",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "international_law",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about international law.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_international_law_continuation": {
            "task": "mmlu_international_law_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "international_law",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about international law.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_jurisprudence": {
            "task": "mmlu_jurisprudence",
            "task_alias": "jurisprudence",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "jurisprudence",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_jurisprudence_continuation": {
            "task": "mmlu_jurisprudence_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "jurisprudence",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about jurisprudence.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_logical_fallacies": {
            "task": "mmlu_logical_fallacies",
            "task_alias": "logical_fallacies",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "logical_fallacies",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_logical_fallacies_continuation": {
            "task": "mmlu_logical_fallacies_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "logical_fallacies",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about logical fallacies.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_machine_learning": {
            "task": "mmlu_machine_learning",
            "task_alias": "machine_learning",
            "tag": "mmlu_stem_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "machine_learning",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_machine_learning_continuation": {
            "task": "mmlu_machine_learning_continuation",
            "tag": "mmlu_stem_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "machine_learning",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about machine learning.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_management": {
            "task": "mmlu_management",
            "task_alias": "management",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "management",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about management.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_management_continuation": {
            "task": "mmlu_management_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "management",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about management.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_marketing": {
            "task": "mmlu_marketing",
            "task_alias": "marketing",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "marketing",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_marketing_continuation": {
            "task": "mmlu_marketing_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "marketing",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about marketing.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_medical_genetics": {
            "task": "mmlu_medical_genetics",
            "task_alias": "medical_genetics",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "medical_genetics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_medical_genetics_continuation": {
            "task": "mmlu_medical_genetics_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "medical_genetics",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about medical genetics.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_miscellaneous": {
            "task": "mmlu_miscellaneous",
            "task_alias": "miscellaneous",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "miscellaneous",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_miscellaneous_continuation": {
            "task": "mmlu_miscellaneous_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "miscellaneous",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about miscellaneous.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_moral_disputes": {
            "task": "mmlu_moral_disputes",
            "task_alias": "moral_disputes",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "moral_disputes",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_moral_disputes_continuation": {
            "task": "mmlu_moral_disputes_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "moral_disputes",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about moral disputes.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_moral_scenarios": {
            "task": "mmlu_moral_scenarios",
            "task_alias": "moral_scenarios",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "moral_scenarios",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_moral_scenarios_continuation": {
            "task": "mmlu_moral_scenarios_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "moral_scenarios",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about moral scenarios.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_nutrition": {
            "task": "mmlu_nutrition",
            "task_alias": "nutrition",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "nutrition",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_nutrition_continuation": {
            "task": "mmlu_nutrition_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "nutrition",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about nutrition.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_philosophy": {
            "task": "mmlu_philosophy",
            "task_alias": "philosophy",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "philosophy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_philosophy_continuation": {
            "task": "mmlu_philosophy_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "philosophy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about philosophy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_prehistory": {
            "task": "mmlu_prehistory",
            "task_alias": "prehistory",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "prehistory",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_prehistory_continuation": {
            "task": "mmlu_prehistory_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "prehistory",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about prehistory.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_accounting": {
            "task": "mmlu_professional_accounting",
            "task_alias": "professional_accounting",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_accounting",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_accounting_continuation": {
            "task": "mmlu_professional_accounting_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_accounting",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional accounting.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_law": {
            "task": "mmlu_professional_law",
            "task_alias": "professional_law",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_law",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_law_continuation": {
            "task": "mmlu_professional_law_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_law",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional law.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_medicine": {
            "task": "mmlu_professional_medicine",
            "task_alias": "professional_medicine",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_medicine",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_medicine_continuation": {
            "task": "mmlu_professional_medicine_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_medicine",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional medicine.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_psychology": {
            "task": "mmlu_professional_psychology",
            "task_alias": "professional_psychology",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_psychology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_professional_psychology_continuation": {
            "task": "mmlu_professional_psychology_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "professional_psychology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about professional psychology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_public_relations": {
            "task": "mmlu_public_relations",
            "task_alias": "public_relations",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "public_relations",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_public_relations_continuation": {
            "task": "mmlu_public_relations_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "public_relations",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about public relations.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_security_studies": {
            "task": "mmlu_security_studies",
            "task_alias": "security_studies",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "security_studies",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_security_studies_continuation": {
            "task": "mmlu_security_studies_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "security_studies",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about security studies.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_sociology": {
            "task": "mmlu_sociology",
            "task_alias": "sociology",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "sociology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_sociology_continuation": {
            "task": "mmlu_sociology_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "sociology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about sociology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_us_foreign_policy": {
            "task": "mmlu_us_foreign_policy",
            "task_alias": "us_foreign_policy",
            "tag": "mmlu_social_sciences_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "us_foreign_policy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_us_foreign_policy_continuation": {
            "task": "mmlu_us_foreign_policy_continuation",
            "tag": "mmlu_social_sciences_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "us_foreign_policy",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about us foreign policy.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_virology": {
            "task": "mmlu_virology",
            "task_alias": "virology",
            "tag": "mmlu_other_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "virology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about virology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_virology_continuation": {
            "task": "mmlu_virology_continuation",
            "tag": "mmlu_other_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "virology",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about virology.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_world_religions": {
            "task": "mmlu_world_religions",
            "task_alias": "world_religions",
            "tag": "mmlu_humanities_tasks",
            "dataset_path": "cais/mmlu",
            "dataset_name": "world_religions",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
            "doc_to_target": "answer",
            "doc_to_choice": [
                "A",
                "B",
                "C",
                "D"
            ],
            "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "mmlu_world_religions_continuation": {
            "task": "mmlu_world_religions_continuation",
            "tag": "mmlu_humanities_continuation",
            "dataset_path": "cais/mmlu",
            "dataset_name": "world_religions",
            "test_split": "test",
            "fewshot_split": "dev",
            "doc_to_text": "Question: {{question.strip()}}\nAnswer:",
            "doc_to_target": "{{answer}}",
            "doc_to_choice": "{{choices}}",
            "description": "The following are questions (with answers) about world religions.\n\n",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "fewshot_config": {
                "sampler": "first_n"
            },
            "num_fewshot": 0,
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": false,
            "metadata": {
                "version": 1.0
            }
        },
        "openbookqa": {
            "task": "openbookqa",
            "dataset_path": "openbookqa",
            "dataset_name": "main",
            "training_split": "train",
            "validation_split": "validation",
            "test_split": "test",
            "doc_to_text": "question_stem",
            "doc_to_target": "{{choices.label.index(answerKey.lstrip())}}",
            "doc_to_choice": "{{choices.text}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "question_stem",
            "metadata": {
                "version": 1.0
            }
        },
        "piqa": {
            "task": "piqa",
            "dataset_path": "baber/piqa",
            "training_split": "train",
            "validation_split": "validation",
            "doc_to_text": "Question: {{goal}}\nAnswer:",
            "doc_to_target": "label",
            "doc_to_choice": "{{[sol1, sol2]}}",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                },
                {
                    "metric": "acc_norm",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "goal",
            "metadata": {
                "version": 1.0
            }
        },
        "winogrande": {
            "task": "winogrande",
            "dataset_path": "winogrande",
            "dataset_name": "winogrande_xl",
            "training_split": "train",
            "validation_split": "validation",
            "doc_to_text": "def doc_to_text(doc):\n    answer_to_num = {\"1\": 0, \"2\": 1}\n    return answer_to_num[doc[\"answer\"]]\n",
            "doc_to_target": "def doc_to_target(doc):\n    idx = doc[\"sentence\"].index(\"_\") + 1\n    return doc[\"sentence\"][idx:].strip()\n",
            "doc_to_choice": "def doc_to_choice(doc):\n    idx = doc[\"sentence\"].index(\"_\")\n    options = [doc[\"option1\"], doc[\"option2\"]]\n    return [doc[\"sentence\"][:idx] + opt for opt in options]\n",
            "description": "",
            "target_delimiter": " ",
            "fewshot_delimiter": "\n\n",
            "num_fewshot": 0,
            "metric_list": [
                {
                    "metric": "acc",
                    "aggregation": "mean",
                    "higher_is_better": true
                }
            ],
            "output_type": "multiple_choice",
            "repeats": 1,
            "should_decontaminate": true,
            "doc_to_decontamination_query": "sentence",
            "metadata": {
                "version": 1.0
            }
        }
    },
    "versions": {
        "arc_challenge": 1.0,
        "arc_easy": 1.0,
        "hellaswag": 1.0,
        "lambada_openai": 1.0,
        "mmlu": 2,
        "mmlu_abstract_algebra": 1.0,
        "mmlu_abstract_algebra_continuation": 1.0,
        "mmlu_anatomy": 1.0,
        "mmlu_anatomy_continuation": 1.0,
        "mmlu_astronomy": 1.0,
        "mmlu_astronomy_continuation": 1.0,
        "mmlu_business_ethics": 1.0,
        "mmlu_business_ethics_continuation": 1.0,
        "mmlu_clinical_knowledge": 1.0,
        "mmlu_clinical_knowledge_continuation": 1.0,
        "mmlu_college_biology": 1.0,
        "mmlu_college_biology_continuation": 1.0,
        "mmlu_college_chemistry": 1.0,
        "mmlu_college_chemistry_continuation": 1.0,
        "mmlu_college_computer_science": 1.0,
        "mmlu_college_computer_science_continuation": 1.0,
        "mmlu_college_mathematics": 1.0,
        "mmlu_college_mathematics_continuation": 1.0,
        "mmlu_college_medicine": 1.0,
        "mmlu_college_medicine_continuation": 1.0,
        "mmlu_college_physics": 1.0,
        "mmlu_college_physics_continuation": 1.0,
        "mmlu_computer_security": 1.0,
        "mmlu_computer_security_continuation": 1.0,
        "mmlu_conceptual_physics": 1.0,
        "mmlu_conceptual_physics_continuation": 1.0,
        "mmlu_continuation": 2,
        "mmlu_econometrics": 1.0,
        "mmlu_econometrics_continuation": 1.0,
        "mmlu_electrical_engineering": 1.0,
        "mmlu_electrical_engineering_continuation": 1.0,
        "mmlu_elementary_mathematics": 1.0,
        "mmlu_elementary_mathematics_continuation": 1.0,
        "mmlu_formal_logic": 1.0,
        "mmlu_formal_logic_continuation": 1.0,
        "mmlu_global_facts": 1.0,
        "mmlu_global_facts_continuation": 1.0,
        "mmlu_high_school_biology": 1.0,
        "mmlu_high_school_biology_continuation": 1.0,
        "mmlu_high_school_chemistry": 1.0,
        "mmlu_high_school_chemistry_continuation": 1.0,
        "mmlu_high_school_computer_science": 1.0,
        "mmlu_high_school_computer_science_continuation": 1.0,
        "mmlu_high_school_european_history": 1.0,
        "mmlu_high_school_european_history_continuation": 1.0,
        "mmlu_high_school_geography": 1.0,
        "mmlu_high_school_geography_continuation": 1.0,
        "mmlu_high_school_government_and_politics": 1.0,
        "mmlu_high_school_government_and_politics_continuation": 1.0,
        "mmlu_high_school_macroeconomics": 1.0,
        "mmlu_high_school_macroeconomics_continuation": 1.0,
        "mmlu_high_school_mathematics": 1.0,
        "mmlu_high_school_mathematics_continuation": 1.0,
        "mmlu_high_school_microeconomics": 1.0,
        "mmlu_high_school_microeconomics_continuation": 1.0,
        "mmlu_high_school_physics": 1.0,
        "mmlu_high_school_physics_continuation": 1.0,
        "mmlu_high_school_psychology": 1.0,
        "mmlu_high_school_psychology_continuation": 1.0,
        "mmlu_high_school_statistics": 1.0,
        "mmlu_high_school_statistics_continuation": 1.0,
        "mmlu_high_school_us_history": 1.0,
        "mmlu_high_school_us_history_continuation": 1.0,
        "mmlu_high_school_world_history": 1.0,
        "mmlu_high_school_world_history_continuation": 1.0,
        "mmlu_human_aging": 1.0,
        "mmlu_human_aging_continuation": 1.0,
        "mmlu_human_sexuality": 1.0,
        "mmlu_human_sexuality_continuation": 1.0,
        "mmlu_humanities": 2,
        "mmlu_international_law": 1.0,
        "mmlu_international_law_continuation": 1.0,
        "mmlu_jurisprudence": 1.0,
        "mmlu_jurisprudence_continuation": 1.0,
        "mmlu_logical_fallacies": 1.0,
        "mmlu_logical_fallacies_continuation": 1.0,
        "mmlu_machine_learning": 1.0,
        "mmlu_machine_learning_continuation": 1.0,
        "mmlu_management": 1.0,
        "mmlu_management_continuation": 1.0,
        "mmlu_marketing": 1.0,
        "mmlu_marketing_continuation": 1.0,
        "mmlu_medical_genetics": 1.0,
        "mmlu_medical_genetics_continuation": 1.0,
        "mmlu_miscellaneous": 1.0,
        "mmlu_miscellaneous_continuation": 1.0,
        "mmlu_moral_disputes": 1.0,
        "mmlu_moral_disputes_continuation": 1.0,
        "mmlu_moral_scenarios": 1.0,
        "mmlu_moral_scenarios_continuation": 1.0,
        "mmlu_nutrition": 1.0,
        "mmlu_nutrition_continuation": 1.0,
        "mmlu_other": 2,
        "mmlu_philosophy": 1.0,
        "mmlu_philosophy_continuation": 1.0,
        "mmlu_prehistory": 1.0,
        "mmlu_prehistory_continuation": 1.0,
        "mmlu_professional_accounting": 1.0,
        "mmlu_professional_accounting_continuation": 1.0,
        "mmlu_professional_law": 1.0,
        "mmlu_professional_law_continuation": 1.0,
        "mmlu_professional_medicine": 1.0,
        "mmlu_professional_medicine_continuation": 1.0,
        "mmlu_professional_psychology": 1.0,
        "mmlu_professional_psychology_continuation": 1.0,
        "mmlu_public_relations": 1.0,
        "mmlu_public_relations_continuation": 1.0,
        "mmlu_security_studies": 1.0,
        "mmlu_security_studies_continuation": 1.0,
        "mmlu_social_sciences": 2,
        "mmlu_sociology": 1.0,
        "mmlu_sociology_continuation": 1.0,
        "mmlu_stem": 2,
        "mmlu_us_foreign_policy": 1.0,
        "mmlu_us_foreign_policy_continuation": 1.0,
        "mmlu_virology": 1.0,
        "mmlu_virology_continuation": 1.0,
        "mmlu_world_religions": 1.0,
        "mmlu_world_religions_continuation": 1.0,
        "openbookqa": 1.0,
        "piqa": 1.0,
        "winogrande": 1.0
    },
    "n-shot": {
        "arc_challenge": 0,
        "arc_easy": 0,
        "hellaswag": 0,
        "lambada_openai": 0,
        "mmlu_abstract_algebra": 0,
        "mmlu_abstract_algebra_continuation": 0,
        "mmlu_anatomy": 0,
        "mmlu_anatomy_continuation": 0,
        "mmlu_astronomy": 0,
        "mmlu_astronomy_continuation": 0,
        "mmlu_business_ethics": 0,
        "mmlu_business_ethics_continuation": 0,
        "mmlu_clinical_knowledge": 0,
        "mmlu_clinical_knowledge_continuation": 0,
        "mmlu_college_biology": 0,
        "mmlu_college_biology_continuation": 0,
        "mmlu_college_chemistry": 0,
        "mmlu_college_chemistry_continuation": 0,
        "mmlu_college_computer_science": 0,
        "mmlu_college_computer_science_continuation": 0,
        "mmlu_college_mathematics": 0,
        "mmlu_college_mathematics_continuation": 0,
        "mmlu_college_medicine": 0,
        "mmlu_college_medicine_continuation": 0,
        "mmlu_college_physics": 0,
        "mmlu_college_physics_continuation": 0,
        "mmlu_computer_security": 0,
        "mmlu_computer_security_continuation": 0,
        "mmlu_conceptual_physics": 0,
        "mmlu_conceptual_physics_continuation": 0,
        "mmlu_econometrics": 0,
        "mmlu_econometrics_continuation": 0,
        "mmlu_electrical_engineering": 0,
        "mmlu_electrical_engineering_continuation": 0,
        "mmlu_elementary_mathematics": 0,
        "mmlu_elementary_mathematics_continuation": 0,
        "mmlu_formal_logic": 0,
        "mmlu_formal_logic_continuation": 0,
        "mmlu_global_facts": 0,
        "mmlu_global_facts_continuation": 0,
        "mmlu_high_school_biology": 0,
        "mmlu_high_school_biology_continuation": 0,
        "mmlu_high_school_chemistry": 0,
        "mmlu_high_school_chemistry_continuation": 0,
        "mmlu_high_school_computer_science": 0,
        "mmlu_high_school_computer_science_continuation": 0,
        "mmlu_high_school_european_history": 0,
        "mmlu_high_school_european_history_continuation": 0,
        "mmlu_high_school_geography": 0,
        "mmlu_high_school_geography_continuation": 0,
        "mmlu_high_school_government_and_politics": 0,
        "mmlu_high_school_government_and_politics_continuation": 0,
        "mmlu_high_school_macroeconomics": 0,
        "mmlu_high_school_macroeconomics_continuation": 0,
        "mmlu_high_school_mathematics": 0,
        "mmlu_high_school_mathematics_continuation": 0,
        "mmlu_high_school_microeconomics": 0,
        "mmlu_high_school_microeconomics_continuation": 0,
        "mmlu_high_school_physics": 0,
        "mmlu_high_school_physics_continuation": 0,
        "mmlu_high_school_psychology": 0,
        "mmlu_high_school_psychology_continuation": 0,
        "mmlu_high_school_statistics": 0,
        "mmlu_high_school_statistics_continuation": 0,
        "mmlu_high_school_us_history": 0,
        "mmlu_high_school_us_history_continuation": 0,
        "mmlu_high_school_world_history": 0,
        "mmlu_high_school_world_history_continuation": 0,
        "mmlu_human_aging": 0,
        "mmlu_human_aging_continuation": 0,
        "mmlu_human_sexuality": 0,
        "mmlu_human_sexuality_continuation": 0,
        "mmlu_international_law": 0,
        "mmlu_international_law_continuation": 0,
        "mmlu_jurisprudence": 0,
        "mmlu_jurisprudence_continuation": 0,
        "mmlu_logical_fallacies": 0,
        "mmlu_logical_fallacies_continuation": 0,
        "mmlu_machine_learning": 0,
        "mmlu_machine_learning_continuation": 0,
        "mmlu_management": 0,
        "mmlu_management_continuation": 0,
        "mmlu_marketing": 0,
        "mmlu_marketing_continuation": 0,
        "mmlu_medical_genetics": 0,
        "mmlu_medical_genetics_continuation": 0,
        "mmlu_miscellaneous": 0,
        "mmlu_miscellaneous_continuation": 0,
        "mmlu_moral_disputes": 0,
        "mmlu_moral_disputes_continuation": 0,
        "mmlu_moral_scenarios": 0,
        "mmlu_moral_scenarios_continuation": 0,
        "mmlu_nutrition": 0,
        "mmlu_nutrition_continuation": 0,
        "mmlu_philosophy": 0,
        "mmlu_philosophy_continuation": 0,
        "mmlu_prehistory": 0,
        "mmlu_prehistory_continuation": 0,
        "mmlu_professional_accounting": 0,
        "mmlu_professional_accounting_continuation": 0,
        "mmlu_professional_law": 0,
        "mmlu_professional_law_continuation": 0,
        "mmlu_professional_medicine": 0,
        "mmlu_professional_medicine_continuation": 0,
        "mmlu_professional_psychology": 0,
        "mmlu_professional_psychology_continuation": 0,
        "mmlu_public_relations": 0,
        "mmlu_public_relations_continuation": 0,
        "mmlu_security_studies": 0,
        "mmlu_security_studies_continuation": 0,
        "mmlu_sociology": 0,
        "mmlu_sociology_continuation": 0,
        "mmlu_us_foreign_policy": 0,
        "mmlu_us_foreign_policy_continuation": 0,
        "mmlu_virology": 0,
        "mmlu_virology_continuation": 0,
        "mmlu_world_religions": 0,
        "mmlu_world_religions_continuation": 0,
        "openbookqa": 0,
        "piqa": 0,
        "winogrande": 0
    },
    "higher_is_better": {
        "arc_challenge": {
            "acc": true,
            "acc_norm": true
        },
        "arc_easy": {
            "acc": true,
            "acc_norm": true
        },
        "hellaswag": {
            "acc": true,
            "acc_norm": true
        },
        "humanities": {
            "acc": true,
            "acc_norm": true
        },
        "lambada_openai": {
            "perplexity": false,
            "acc": true
        },
        "mmlu": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_abstract_algebra": {
            "acc": true
        },
        "mmlu_abstract_algebra_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_anatomy": {
            "acc": true
        },
        "mmlu_anatomy_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_astronomy": {
            "acc": true
        },
        "mmlu_astronomy_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_business_ethics": {
            "acc": true
        },
        "mmlu_business_ethics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_clinical_knowledge": {
            "acc": true
        },
        "mmlu_clinical_knowledge_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_biology": {
            "acc": true
        },
        "mmlu_college_biology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_chemistry": {
            "acc": true
        },
        "mmlu_college_chemistry_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_computer_science": {
            "acc": true
        },
        "mmlu_college_computer_science_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_mathematics": {
            "acc": true
        },
        "mmlu_college_mathematics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_medicine": {
            "acc": true
        },
        "mmlu_college_medicine_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_college_physics": {
            "acc": true
        },
        "mmlu_college_physics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_computer_security": {
            "acc": true
        },
        "mmlu_computer_security_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_conceptual_physics": {
            "acc": true
        },
        "mmlu_conceptual_physics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_econometrics": {
            "acc": true
        },
        "mmlu_econometrics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_electrical_engineering": {
            "acc": true
        },
        "mmlu_electrical_engineering_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_elementary_mathematics": {
            "acc": true
        },
        "mmlu_elementary_mathematics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_formal_logic": {
            "acc": true
        },
        "mmlu_formal_logic_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_global_facts": {
            "acc": true
        },
        "mmlu_global_facts_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_biology": {
            "acc": true
        },
        "mmlu_high_school_biology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_chemistry": {
            "acc": true
        },
        "mmlu_high_school_chemistry_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_computer_science": {
            "acc": true
        },
        "mmlu_high_school_computer_science_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_european_history": {
            "acc": true
        },
        "mmlu_high_school_european_history_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_geography": {
            "acc": true
        },
        "mmlu_high_school_geography_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_government_and_politics": {
            "acc": true
        },
        "mmlu_high_school_government_and_politics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_macroeconomics": {
            "acc": true
        },
        "mmlu_high_school_macroeconomics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_mathematics": {
            "acc": true
        },
        "mmlu_high_school_mathematics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_microeconomics": {
            "acc": true
        },
        "mmlu_high_school_microeconomics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_physics": {
            "acc": true
        },
        "mmlu_high_school_physics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_psychology": {
            "acc": true
        },
        "mmlu_high_school_psychology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_statistics": {
            "acc": true
        },
        "mmlu_high_school_statistics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_us_history": {
            "acc": true
        },
        "mmlu_high_school_us_history_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_high_school_world_history": {
            "acc": true
        },
        "mmlu_high_school_world_history_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_human_aging": {
            "acc": true
        },
        "mmlu_human_aging_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_human_sexuality": {
            "acc": true
        },
        "mmlu_human_sexuality_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_humanities": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_international_law": {
            "acc": true
        },
        "mmlu_international_law_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_jurisprudence": {
            "acc": true
        },
        "mmlu_jurisprudence_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_logical_fallacies": {
            "acc": true
        },
        "mmlu_logical_fallacies_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_machine_learning": {
            "acc": true
        },
        "mmlu_machine_learning_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_management": {
            "acc": true
        },
        "mmlu_management_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_marketing": {
            "acc": true
        },
        "mmlu_marketing_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_medical_genetics": {
            "acc": true
        },
        "mmlu_medical_genetics_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_miscellaneous": {
            "acc": true
        },
        "mmlu_miscellaneous_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_moral_disputes": {
            "acc": true
        },
        "mmlu_moral_disputes_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_moral_scenarios": {
            "acc": true
        },
        "mmlu_moral_scenarios_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_nutrition": {
            "acc": true
        },
        "mmlu_nutrition_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_other": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_philosophy": {
            "acc": true
        },
        "mmlu_philosophy_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_prehistory": {
            "acc": true
        },
        "mmlu_prehistory_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_professional_accounting": {
            "acc": true
        },
        "mmlu_professional_accounting_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_professional_law": {
            "acc": true
        },
        "mmlu_professional_law_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_professional_medicine": {
            "acc": true
        },
        "mmlu_professional_medicine_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_professional_psychology": {
            "acc": true
        },
        "mmlu_professional_psychology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_public_relations": {
            "acc": true
        },
        "mmlu_public_relations_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_security_studies": {
            "acc": true
        },
        "mmlu_security_studies_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_social_sciences": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_sociology": {
            "acc": true
        },
        "mmlu_sociology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_stem": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_us_foreign_policy": {
            "acc": true
        },
        "mmlu_us_foreign_policy_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_virology": {
            "acc": true
        },
        "mmlu_virology_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "mmlu_world_religions": {
            "acc": true
        },
        "mmlu_world_religions_continuation": {
            "acc": true,
            "acc_norm": true
        },
        "openbookqa": {
            "acc": true,
            "acc_norm": true
        },
        "other": {
            "acc": true,
            "acc_norm": true
        },
        "piqa": {
            "acc": true,
            "acc_norm": true
        },
        "social sciences": {
            "acc": true,
            "acc_norm": true
        },
        "stem": {
            "acc": true,
            "acc_norm": true
        },
        "winogrande": {
            "acc": true
        }
    },
    "n-samples": {
        "winogrande": {
            "original": 1267,
            "effective": 1267
        },
        "piqa": {
            "original": 1838,
            "effective": 1838
        },
        "openbookqa": {
            "original": 500,
            "effective": 500
        },
        "mmlu_abstract_algebra_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_anatomy_continuation": {
            "original": 135,
            "effective": 135
        },
        "mmlu_astronomy_continuation": {
            "original": 152,
            "effective": 152
        },
        "mmlu_college_biology_continuation": {
            "original": 144,
            "effective": 144
        },
        "mmlu_college_chemistry_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_computer_science_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_mathematics_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_physics_continuation": {
            "original": 102,
            "effective": 102
        },
        "mmlu_computer_security_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_conceptual_physics_continuation": {
            "original": 235,
            "effective": 235
        },
        "mmlu_electrical_engineering_continuation": {
            "original": 145,
            "effective": 145
        },
        "mmlu_elementary_mathematics_continuation": {
            "original": 378,
            "effective": 378
        },
        "mmlu_high_school_biology_continuation": {
            "original": 310,
            "effective": 310
        },
        "mmlu_high_school_chemistry_continuation": {
            "original": 203,
            "effective": 203
        },
        "mmlu_high_school_computer_science_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_high_school_mathematics_continuation": {
            "original": 270,
            "effective": 270
        },
        "mmlu_high_school_physics_continuation": {
            "original": 151,
            "effective": 151
        },
        "mmlu_high_school_statistics_continuation": {
            "original": 216,
            "effective": 216
        },
        "mmlu_machine_learning_continuation": {
            "original": 112,
            "effective": 112
        },
        "mmlu_business_ethics_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_clinical_knowledge_continuation": {
            "original": 265,
            "effective": 265
        },
        "mmlu_college_medicine_continuation": {
            "original": 173,
            "effective": 173
        },
        "mmlu_global_facts_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_human_aging_continuation": {
            "original": 223,
            "effective": 223
        },
        "mmlu_management_continuation": {
            "original": 103,
            "effective": 103
        },
        "mmlu_marketing_continuation": {
            "original": 234,
            "effective": 234
        },
        "mmlu_medical_genetics_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_miscellaneous_continuation": {
            "original": 783,
            "effective": 783
        },
        "mmlu_nutrition_continuation": {
            "original": 306,
            "effective": 306
        },
        "mmlu_professional_accounting_continuation": {
            "original": 282,
            "effective": 282
        },
        "mmlu_professional_medicine_continuation": {
            "original": 272,
            "effective": 272
        },
        "mmlu_virology_continuation": {
            "original": 166,
            "effective": 166
        },
        "mmlu_econometrics_continuation": {
            "original": 114,
            "effective": 114
        },
        "mmlu_high_school_geography_continuation": {
            "original": 198,
            "effective": 198
        },
        "mmlu_high_school_government_and_politics_continuation": {
            "original": 193,
            "effective": 193
        },
        "mmlu_high_school_macroeconomics_continuation": {
            "original": 390,
            "effective": 390
        },
        "mmlu_high_school_microeconomics_continuation": {
            "original": 238,
            "effective": 238
        },
        "mmlu_high_school_psychology_continuation": {
            "original": 545,
            "effective": 545
        },
        "mmlu_human_sexuality_continuation": {
            "original": 131,
            "effective": 131
        },
        "mmlu_professional_psychology_continuation": {
            "original": 612,
            "effective": 612
        },
        "mmlu_public_relations_continuation": {
            "original": 110,
            "effective": 110
        },
        "mmlu_security_studies_continuation": {
            "original": 245,
            "effective": 245
        },
        "mmlu_sociology_continuation": {
            "original": 201,
            "effective": 201
        },
        "mmlu_us_foreign_policy_continuation": {
            "original": 100,
            "effective": 100
        },
        "mmlu_formal_logic_continuation": {
            "original": 126,
            "effective": 126
        },
        "mmlu_high_school_european_history_continuation": {
            "original": 165,
            "effective": 165
        },
        "mmlu_high_school_us_history_continuation": {
            "original": 204,
            "effective": 204
        },
        "mmlu_high_school_world_history_continuation": {
            "original": 237,
            "effective": 237
        },
        "mmlu_international_law_continuation": {
            "original": 121,
            "effective": 121
        },
        "mmlu_jurisprudence_continuation": {
            "original": 108,
            "effective": 108
        },
        "mmlu_logical_fallacies_continuation": {
            "original": 163,
            "effective": 163
        },
        "mmlu_moral_disputes_continuation": {
            "original": 346,
            "effective": 346
        },
        "mmlu_moral_scenarios_continuation": {
            "original": 895,
            "effective": 895
        },
        "mmlu_philosophy_continuation": {
            "original": 311,
            "effective": 311
        },
        "mmlu_prehistory_continuation": {
            "original": 324,
            "effective": 324
        },
        "mmlu_professional_law_continuation": {
            "original": 1534,
            "effective": 1534
        },
        "mmlu_world_religions_continuation": {
            "original": 171,
            "effective": 171
        },
        "mmlu_abstract_algebra": {
            "original": 100,
            "effective": 100
        },
        "mmlu_anatomy": {
            "original": 135,
            "effective": 135
        },
        "mmlu_astronomy": {
            "original": 152,
            "effective": 152
        },
        "mmlu_college_biology": {
            "original": 144,
            "effective": 144
        },
        "mmlu_college_chemistry": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_computer_science": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_mathematics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_college_physics": {
            "original": 102,
            "effective": 102
        },
        "mmlu_computer_security": {
            "original": 100,
            "effective": 100
        },
        "mmlu_conceptual_physics": {
            "original": 235,
            "effective": 235
        },
        "mmlu_electrical_engineering": {
            "original": 145,
            "effective": 145
        },
        "mmlu_elementary_mathematics": {
            "original": 378,
            "effective": 378
        },
        "mmlu_high_school_biology": {
            "original": 310,
            "effective": 310
        },
        "mmlu_high_school_chemistry": {
            "original": 203,
            "effective": 203
        },
        "mmlu_high_school_computer_science": {
            "original": 100,
            "effective": 100
        },
        "mmlu_high_school_mathematics": {
            "original": 270,
            "effective": 270
        },
        "mmlu_high_school_physics": {
            "original": 151,
            "effective": 151
        },
        "mmlu_high_school_statistics": {
            "original": 216,
            "effective": 216
        },
        "mmlu_machine_learning": {
            "original": 112,
            "effective": 112
        },
        "mmlu_business_ethics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_clinical_knowledge": {
            "original": 265,
            "effective": 265
        },
        "mmlu_college_medicine": {
            "original": 173,
            "effective": 173
        },
        "mmlu_global_facts": {
            "original": 100,
            "effective": 100
        },
        "mmlu_human_aging": {
            "original": 223,
            "effective": 223
        },
        "mmlu_management": {
            "original": 103,
            "effective": 103
        },
        "mmlu_marketing": {
            "original": 234,
            "effective": 234
        },
        "mmlu_medical_genetics": {
            "original": 100,
            "effective": 100
        },
        "mmlu_miscellaneous": {
            "original": 783,
            "effective": 783
        },
        "mmlu_nutrition": {
            "original": 306,
            "effective": 306
        },
        "mmlu_professional_accounting": {
            "original": 282,
            "effective": 282
        },
        "mmlu_professional_medicine": {
            "original": 272,
            "effective": 272
        },
        "mmlu_virology": {
            "original": 166,
            "effective": 166
        },
        "mmlu_econometrics": {
            "original": 114,
            "effective": 114
        },
        "mmlu_high_school_geography": {
            "original": 198,
            "effective": 198
        },
        "mmlu_high_school_government_and_politics": {
            "original": 193,
            "effective": 193
        },
        "mmlu_high_school_macroeconomics": {
            "original": 390,
            "effective": 390
        },
        "mmlu_high_school_microeconomics": {
            "original": 238,
            "effective": 238
        },
        "mmlu_high_school_psychology": {
            "original": 545,
            "effective": 545
        },
        "mmlu_human_sexuality": {
            "original": 131,
            "effective": 131
        },
        "mmlu_professional_psychology": {
            "original": 612,
            "effective": 612
        },
        "mmlu_public_relations": {
            "original": 110,
            "effective": 110
        },
        "mmlu_security_studies": {
            "original": 245,
            "effective": 245
        },
        "mmlu_sociology": {
            "original": 201,
            "effective": 201
        },
        "mmlu_us_foreign_policy": {
            "original": 100,
            "effective": 100
        },
        "mmlu_formal_logic": {
            "original": 126,
            "effective": 126
        },
        "mmlu_high_school_european_history": {
            "original": 165,
            "effective": 165
        },
        "mmlu_high_school_us_history": {
            "original": 204,
            "effective": 204
        },
        "mmlu_high_school_world_history": {
            "original": 237,
            "effective": 237
        },
        "mmlu_international_law": {
            "original": 121,
            "effective": 121
        },
        "mmlu_jurisprudence": {
            "original": 108,
            "effective": 108
        },
        "mmlu_logical_fallacies": {
            "original": 163,
            "effective": 163
        },
        "mmlu_moral_disputes": {
            "original": 346,
            "effective": 346
        },
        "mmlu_moral_scenarios": {
            "original": 895,
            "effective": 895
        },
        "mmlu_philosophy": {
            "original": 311,
            "effective": 311
        },
        "mmlu_prehistory": {
            "original": 324,
            "effective": 324
        },
        "mmlu_professional_law": {
            "original": 1534,
            "effective": 1534
        },
        "mmlu_world_religions": {
            "original": 171,
            "effective": 171
        },
        "lambada_openai": {
            "original": 5153,
            "effective": 5153
        },
        "hellaswag": {
            "original": 10042,
            "effective": 10042
        },
        "arc_easy": {
            "original": 2376,
            "effective": 2376
        },
        "arc_challenge": {
            "original": 1172,
            "effective": 1172
        }
    },
    "config": {
        "model": "/root/mixture_of_recursions/hf_models/SmolLM-360M",
        "model_args": "pretrained=/root/mixture_of_recursions/results/pretrain/250821_pretrain_smollm-360m_rec2_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001,trust_remote_code=True",
        "model_num_parameters": 134516160,
        "model_dtype": "torch.bfloat16",
        "model_revision": "main",
        "model_sha": "",
        "batch_size": 32,
        "batch_sizes": [],
        "device": "cuda:0",
        "use_cache": null,
        "limit": null,
        "bootstrap_iters": 100000,
        "gen_kwargs": null,
        "random_seed": 0,
        "numpy_seed": 1234,
        "torch_seed": 1234,
        "fewshot_seed": 1234
    },
    "git_hash": "d2fd900",
    "date": 1755942265.955561,
    "pretty_env_info": "PyTorch version: 2.8.0+cu128\nIs debug build: False\nCUDA used to build PyTorch: 12.8\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.4 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:09:17) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.4.131\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA L20\nNvidia driver version: 550.54.14\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      52 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             180\nOn-line CPU(s) list:                0-179\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8457C\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 45\nSocket(s):                          2\nStepping:                           8\nBogoMIPS:                           5200.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 arch_capabilities\nHypervisor vendor:                  KVM\nVirtualization type:                full\nL1d cache:                          4.2 MiB (90 instances)\nL1i cache:                          2.8 MiB (90 instances)\nL2 cache:                           180 MiB (90 instances)\nL3 cache:                           195 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-89\nNUMA node1 CPU(s):                  90-179\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Unknown: No mitigations\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Mitigation; TSX disabled\n\nVersions of relevant libraries:\n[pip3] numpy==2.3.2\n[pip3] nvidia-cublas-cu12==12.8.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.8.90\n[pip3] nvidia-cuda-nvrtc-cu12==12.8.93\n[pip3] nvidia-cuda-runtime-cu12==12.8.90\n[pip3] nvidia-cudnn-cu12==9.10.2.21\n[pip3] nvidia-cufft-cu12==11.3.3.83\n[pip3] nvidia-curand-cu12==10.3.9.90\n[pip3] nvidia-cusolver-cu12==11.7.3.90\n[pip3] nvidia-cusparse-cu12==12.5.8.93\n[pip3] nvidia-cusparselt-cu12==0.7.1\n[pip3] nvidia-nccl-cu12==2.27.3\n[pip3] nvidia-nvjitlink-cu12==12.8.93\n[pip3] nvidia-nvtx-cu12==12.8.90\n[pip3] torch==2.8.0\n[pip3] triton==3.4.0\n[conda] numpy                     2.3.2                    pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.8.4.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.8.90                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.8.93                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.8.90                  pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.10.2.21                pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.3.3.83                pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.9.90                pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.7.3.90                pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.5.8.93                pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.7.1                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.27.3                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.8.93                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.8.90                  pypi_0    pypi\n[conda] torch                     2.8.0                    pypi_0    pypi\n[conda] triton                    3.4.0                    pypi_0    pypi",
    "transformers_version": "4.52.4",
    "upper_git_hash": null,
    "tokenizer_pad_token": [
        "<|endoftext|>",
        "0"
    ],
    "tokenizer_eos_token": [
        "<|endoftext|>",
        "0"
    ],
    "tokenizer_bos_token": [
        "<|endoftext|>",
        "0"
    ],
    "eot_token_id": 0,
    "max_length": 2048,
    "task_hashes": {},
    "model_source": "recursive_lm",
    "model_name": "/root/mixture_of_recursions/results/pretrain/250821_pretrain_smollm-360m_rec2_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001",
    "model_name_sanitized": "__root__mixture_of_recursions__results__pretrain__250821_pretrain_smollm-360m_rec2_middle_cycle_random_lr3e-3_mor_expert_linear_alpha_0.1_sigmoid_aux_loss_0.001",
    "system_instruction": null,
    "system_instruction_sha": null,
    "fewshot_as_multiturn": false,
    "chat_template": null,
    "chat_template_sha": null,
    "start_time": 42785701.49622922,
    "end_time": 42786296.97165843,
    "total_evaluation_time_seconds": "595.4754292145371"
}